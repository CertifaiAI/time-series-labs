{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beautiful-world",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "![logo](../../picture/license_header_logo.png)\n",
    "> **Copyright &copy; 2020 - 2021 CertifAI Sdn. Bhd.**<br>\n",
    " <br>\n",
    "This program and the accompanying materials are made available under the\n",
    "terms of the [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0). <br>\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n",
    "WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n",
    "License for the specific language governing permissions and limitations\n",
    "under the License. <br>\n",
    "<br>**SPDX-License-Identifier: Apache-2.0**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "altered-holiday",
   "metadata": {},
   "source": [
    "# 03 - Recap Exercise \n",
    "\n",
    "## Requirement\n",
    "You should complete all the exercises from Day 1 to Day 3 before proceeding with this recap exercise to optimize your learning experience. \n",
    "\n",
    "\n",
    "## Introduction \n",
    "This tutorial is to demonstrate the process flow of building time series forecast algorithms from scratch.<br>\n",
    "\n",
    ">**Example of time series forecasting flow diagram:**\n",
    "![image](https://user-images.githubusercontent.com/59526258/117595686-0f8aa800-b174-11eb-8996-b08559378fad.png)\n",
    "\n",
    "This exercise will use five different types of time series forecast model to solve the problem:\n",
    "\n",
    "1. Naive Forecast\n",
    "2. Exponential Smoothing Average\n",
    "3. ARIMA \n",
    "4. SARIMA\n",
    "5. Multilayer perceptron (MLP)\n",
    "\n",
    "## Problem Statement\n",
    "You are given a set of the dataset that measures milk production(pounds per cow) as per month from January 1962 to December 1975. You are required to build a time series forecast algorithm from scratch. Your algorithm must include:\n",
    "1. Basic analytics of the data.\n",
    "2. Time series modeling with statistical method.\n",
    "3. Time series modeling with deep learning method.\n",
    "4. Compare and choose the best model base on the performance.\n",
    "\n",
    "## What will we accomplish?\n",
    "By the end of this tutorial, you will be able to:|\n",
    "1. Understand the time series forecasting flow.\n",
    "2. Compare and select the optimal model as forecasting model base on the model performance. \n",
    "\n",
    "\n",
    "## Notebook Outline\n",
    "Below is the outline for this tutorial\n",
    "1. [Basic Analytics](#BasicAnalytics)\n",
    "    * [Data Preparation](#DataPreparation)\n",
    "    * [Data Visualization](#DataVisualization)\n",
    "    * [Data Splitting](#DataSplitting)\n",
    "    * [ACF Plot](#ACFPlot)\n",
    "    * [Time Series Decomposition](#TimeSeriesDecomposition)\n",
    "    \n",
    "2. [Time Series Modeling with Statistical Method](#TimeSeriesModelingwithStatisticalMethod)\n",
    "    * [Naive Forecast](#NaiveForecast)\n",
    "    * [Exponential Moving Average](#ExponentialMovingAverage)\n",
    "        * [Simple Exponential Moving Average (SEMA)](#SimpleExponentialMovingAverage(SEMA))\n",
    "        * [Holt-Winters Exponential Moving Average Method](#Holt-WintersMethod)\n",
    "    * [ARIMA Forecast](#ARIMAForecast)\n",
    "        * [Log Transform](#LogTransform)\n",
    "        * [Seasonal Differencing (Deseasonalize)](#SeasonalDifferencing(Deseasonalize))\n",
    "        * [ADF Test](#ADFTest)\n",
    "        * [1st order differencing (Detrending)](#1storderdifferencing(Detrending))\n",
    "        * [ACF and PACF plot](#ACFandPACFplot)\n",
    "        * [ARIMA model configuration](#ARIMAmodelconfiguration)\n",
    "        * [ARIMA model forecast](#ARIMAmodelforecast)\n",
    "        * [Reverse Differencing](#ReverseDifferencing)\n",
    "            * [Reverse 1st order differencing](#Reverse1storderdifferencing)\n",
    "            * [Reverse seasonal differencing](#Reverseseasonaldifferencing)\n",
    "        * [Inverse Log Transform](#InverseLogTransform)\n",
    "    * [SARIMA Forecast](#SARIMAForecast)\n",
    "    \n",
    "3. [Time Series Modeling with Deep Learning Method (MLP)](#TimeSeriesModelingwithDeepLearningMethod(MLP))\n",
    "    * [Hyperparamter](#Hyperparamter)\n",
    "    * [Data Scaling](#DataScaling)\n",
    "    * [Window Sliding](#WindowSliding)\n",
    "    * [Data Iterator](#DataIterator)\n",
    "    * [Multilayer perceptron (MLP) configuration](#Multilayerperceptron(MLP)configuration)\n",
    "    * [Input Model](#InputModel)\n",
    "    * [Model Summary](#ModelSummary)\n",
    "    * [Training](#Training)\n",
    "    * [Validation](#Validation)\n",
    "4. [Summary](#Summary)\n",
    "5. [Reference](#Reference)\n",
    "\n",
    "First, let's import the package needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broken-vermont",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import seaborn as sns\n",
    "import math\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statsmodels.graphics.tsaplots import plot_acf , plot_pacf\n",
    "from statsmodels.tsa.holtwinters import Holt, ExponentialSmoothing, SimpleExpSmoothing\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from matplotlib import pylab\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "%matplotlib inline \n",
    "pylab.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alien-dancing",
   "metadata": {},
   "source": [
    "## <a name=\"BasicAnalytics\">1. Basic Analytics</a>\n",
    "### <a name=\"DataPreparation\">1.1 - Data Preparation\n",
    "In data preparation, you are required to read the data and make the `Month` as the index.\n",
    ">**Instruction:**<br>\n",
    "Change the data frame index as `Month` and set the frequency as `MS` using `df.index.freq`<br>\n",
    "\n",
    ">**Expected Result:**<br>\n",
    "Example of first 5 row data \n",
    "\n",
    "Month| Monthly milk production (pounds per cow)\n",
    "---|---\n",
    "1962-01-01|\t589\n",
    "1962-02-01|\t561\n",
    "1962-03-01|\t640\n",
    "1962-04-01|\t656\n",
    "1962-05-01|\t727"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-emission",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV data\n",
    "milk_data = pd.read_csv(\"https://raw.githubusercontent.com/plotly/datasets/master/monthly-milk-production-pounds.csv\")\n",
    "milk_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "piano-judges",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d5631acab7416f2eda84cca797450e82",
     "grade": false,
     "grade_id": "cell-de31d276ba31fd22",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Set \"Month\" as data index\n",
    "# YOUR CODE HERE\n",
    "\n",
    "milk_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portable-appearance",
   "metadata": {},
   "source": [
    "### <a name=\"DataVisualization\">1.2 - Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strong-genius",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "milk_data.plot()\n",
    "plt.title(\"Montly Milk Production\")\n",
    "plt.ylabel(\"Pounds per cow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-superior",
   "metadata": {},
   "source": [
    "As you can notice, the data have shown that it has increased in trend and it contains seasonality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "humanitarian-visibility",
   "metadata": {},
   "source": [
    "### <a name=\"DataSplitting\">1.3 - Data Splitting\n",
    "Split the data into train and test data using `train_test_split`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absent-melissa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Splitting\n",
    "split_ratio = 0.7\n",
    "train_data, test_data = train_test_split(milk_data, train_size=split_ratio , shuffle = False)\n",
    "train_time,test_time = train_data.index, test_data.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "announced-tribute",
   "metadata": {},
   "source": [
    "Before we build the model, we must analyze the time series data pattern. There are two ways to visualize the data seasonality:\n",
    "1. ACF plot\n",
    "2. Time Series Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infectious-tower",
   "metadata": {},
   "source": [
    "### <a name=\"ACFPlot\"> 1.4 - ACF Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partial-tower",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ACF Plot\n",
    "plot_acf(train_data,lags = 60)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "looking-drama",
   "metadata": {},
   "source": [
    "### <a name=\"TimeSeriesDecomposition\">1.5 - Time Series Decomposition\n",
    "Decomposition gives us more details about the time series data pattern by decomposing the data into trend, seasonality, and residual.\n",
    ">**Instruction:**<br>\n",
    "Choose the correct decomposition parameters and perform time series decomposition to the `train_data`. Save the output with a variable name `decomposition`.\n",
    "\n",
    ">**Expected Result:**<br>\n",
    "![image](https://user-images.githubusercontent.com/59526258/117621580-52647400-b1a4-11eb-9590-96ed7fc97198.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historical-sphere",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a42143e3b0639c2cafa20e0fde6caa9a",
     "grade": false,
     "grade_id": "cell-42f9bd60600e5132",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Time Series Decomposition\n",
    "# YOUR CODE HERE\n",
    "\n",
    "decomposition.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mental-intro",
   "metadata": {},
   "source": [
    "## <a name=\"TimeSeriesModelingwithStatisticalMethod\">2. Time Series Modeling with Statistical Method\n",
    "### <a name=\"NaiveForecast\">2.1 - Naive Forecast\n",
    "First, we will use the naive forecast method as our benchmark model. We only accept those model that is out performing than the naive forecast model based on the result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unable-buffalo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive forecast method \n",
    "naive_forecast = test_data.shift(1)\n",
    "\n",
    "# Function to plot the forecast data\n",
    "def forecast_plot(forecast_data,forecast_label, test_label='Test Data',test_time = test_time,test_data= test_data):\n",
    "    plt.figure(figsize=(15,6))\n",
    "    plt.plot(test_time,forecast_data,'r',label = forecast_label,)\n",
    "    plt.plot(test_time,test_data,label = test_label)\n",
    "    plt.legend()\n",
    "    plt.title(\"Montly Milk Production Forecast\")\n",
    "    plt.xlabel(\"Month\")\n",
    "    plt.ylabel(\"Pounds per cow\")\n",
    "    \n",
    "# Plot the forecast data\n",
    "forecast_plot(forecast_data = naive_forecast,forecast_label = 'Naive Forecast')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informal-shakespeare",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RMSE of naive model\n",
    "testScore_naive = math.sqrt(mean_squared_error(test_data[1:], naive_forecast[1:]))\n",
    "testScore_naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tropical-woman",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the result into Dataframe\n",
    "result = pd.DataFrame({'Naive Forecast' :testScore_naive},index=[\"RMSE\"])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "packed-radius",
   "metadata": {},
   "source": [
    "### <a name=\"ExponentialMovingAverage\">2.2 - Exponential Moving Average \n",
    "#### <a name=\"SimpleExponentialMovingAverage(SEMA)\">2.2.1 - Simple Exponential Moving Average (SEMA)\n",
    "Let's start with the Simple Exponential Moving Average (SEMA) to determine whether this method is the correct method to use as our forecasting model.\n",
    ">**Instruction:**<br>\n",
    "Use `SimpleExpSmoothing` as the forecast model and fit it with `train_data`. Your forecast data must be same length as the test_data. Save the return with a variable name with `sema_forecast`\n",
    "\n",
    ">**Expected Result:**<br>\n",
    "![image](https://user-images.githubusercontent.com/59526258/117624498-8f7e3580-b1a7-11eb-8f46-781b87644a0e.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "social-master",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "957a225d641d7875effd2eb7be3ecfbe",
     "grade": false,
     "grade_id": "cell-5cfc5fa7d6005c28",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Simple Exponential Moving Average (SEMA)\n",
    "# YOUR CODE HERE\n",
    "\n",
    "sema_forecast.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functional-edition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast data plot\n",
    "forecast_plot(forecast_data = sema_forecast,forecast_label = 'SEMA Forecast' )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charming-restriction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the result into Dataframe\n",
    "sema_forecast_result = math.sqrt(mean_squared_error(sema_forecast,test_data))\n",
    "result['SEMA'] =  sema_forecast_result \n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electoral-indiana",
   "metadata": {},
   "source": [
    "It seems like Simple Exponential Moving Average is not a good model for the data with seasonality. Let's try the Holt-Winters Method because the model is designed to handle the time series data with seasonality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pharmaceutical-costa",
   "metadata": {},
   "source": [
    "#### <a name=\"Holt-WintersMethod\">2.2.2 - Holt-Winters Exponential Moving Average Method\n",
    ">**Instruction:**<br>\n",
    "Perform both additive and multiplicative Holt-Winters Method with the `train_data` and save the model result into `result`. The number of forecast data must be the same size as the `test_data`. \n",
    "\n",
    ">*Hints: You may use For Loop to assist you with the iterate of parameters such as trend and seasonal.* <br>\n",
    "Example:<br>\n",
    "method = ['add','mul']<br>\n",
    "&ensp; for trend in method:<br>\n",
    "&ensp;&ensp;     for seasonal in method:<br>\n",
    "&ensp;&ensp;&ensp;    ----<br>\n",
    "&ensp;&ensp;&ensp;    ----<br>\n",
    "&ensp;&ensp;&ensp;    ----<br>\n",
    "\n",
    ">**Expected Result:**<br>\n",
    "\n",
    "---|Naive Forecast|\tSEMA|\tadd_add_forecast|\tadd_mul_forecast|\tmul_add_forecast|\tmul_mul_forecast\n",
    "---|---|\t---|\t---|\t---|\t---|\t---\n",
    "RMSE|\t47.390505|\t85.317285|\t51.227547|\t43.414228|\t65.745351|\t56.389882\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boolean-offering",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "278a01ee6ae09f42f4ae8fc2143b16dc",
     "grade": false,
     "grade_id": "cell-b4f20e57e7cc3248",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Define Holt-Winters Exponential Moving Average model\n",
    "# YOUR CODE HERE\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frequent-participant",
   "metadata": {},
   "source": [
    "The result shows that Holt-Winters Method with the additive trend and multiplicative seasonal parameter is the best. Let's visualize the forecast data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "union-federation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use additive trend and multiplicative seasonal Holt-Winters method to forecast\n",
    "exponential = ExponentialSmoothing(train_data, seasonal_periods=12, trend='add', seasonal='mul').fit()\n",
    "exponential_forecast = exponential.forecast(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic-fault",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exponential forecast plot\n",
    "forecast_plot(forecast_data = exponential_forecast,forecast_label = 'Holt-Winters Method Forecast')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optimum-output",
   "metadata": {},
   "source": [
    "### <a name=\"ARIMAForecast\">2.3 - ARIMA Forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artificial-trade",
   "metadata": {},
   "source": [
    "#### <a name=\"LogTransform\">2.3.1 - Log Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reflected-inquiry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log Transform \n",
    "train_data_log = np.log(train_data)\n",
    "train_data_log.plot()\n",
    "plt.title(\"Logged Montly Milk Production Forecast\")\n",
    "plt.ylabel(\"Pounds per cow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "existing-outside",
   "metadata": {},
   "source": [
    "#### <a name=\"SeasonalDifferencing(Deseasonalize)\">2.3.2 -  Seasonal Differencing (Deseasonalize)\n",
    "`ARIMA` is only good at handling the data without seasonality. Let's remove the data seasonality by using the differencing technique with a `differencing_month = 12`. The `differencing_month` is determined base on the seasonality period of the data.\n",
    "\n",
    "**How seasonal differencing work?**<br>\n",
    "The table below shown the calculation process of seasonal differencing (seasonal period=2). \n",
    "> The seasonal period can be any values base on the nature of the time series data seasonal period.\n",
    "\n",
    "After shifting the data down to two columns (due to seasonal period=2), perform subtraction to the `Data` and the shifted data (`Shift(2)`). Your final result will be the data that has been remove seasonality.\n",
    "![concept](../../picture/seasonal_difference.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conservative-finger",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seasonal Differencing\n",
    "differencing_month = 12 \n",
    "remove_seasonal = train_data_log.diff(differencing_month)\n",
    "deseasonal_data = remove_seasonal[differencing_month:]\n",
    "deseasonal_data.plot()\n",
    "plt.title(\"Deseasonalize Montly Milk Production Forecast\")\n",
    "plt.ylabel(\"Pounds per cow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abstract-rebel",
   "metadata": {},
   "source": [
    "After removing the seasonality, it is essential to perform an `ADF` test to check whether the data is achieved through stationary data. The data after seasonal differencing still have the trend properties, which will make the data non-stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lyric-italy",
   "metadata": {},
   "source": [
    "#### <a name=\"ADFTest\"> 2.3.3 - ADF Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facial-poster",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADF Test\n",
    "def print_adf_result(adf_result):\n",
    "    df_results = pd.Series(adf_result[0:4], index=['ADF Test Statistic','P-Value','# Lags Used','# Observations Used'])\n",
    "    \n",
    "    for key, value in adf_result[4].items():\n",
    "        df_results['Critical Value (%s)'% key] = value\n",
    "    print('Augmented Dickey-Fuller Test Results:')\n",
    "    print(df_results)\n",
    "    \n",
    "\n",
    "adf_result = adfuller(deseasonal_data, maxlag=12)\n",
    "print_adf_result(adf_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amber-tournament",
   "metadata": {},
   "source": [
    "It seems that the data is not stationary yet. You are required to perform 1st order differencing to make the data stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surface-hardwood",
   "metadata": {},
   "source": [
    "####  <a name=\"1storderdifferencing(Detrending)\">2.3.4 - 1st order differencing (Detrending)\n",
    "**How 1st order differencing work?**<br>\n",
    "The concept is similar to seasonal differencing. The only difference is 1st order differencing will always perform subtraction to the shifted data with period=1.<br>\n",
    "For Example:<br>\n",
    "![detrending_concept](../../picture/TS_detrending_concept.png)\n",
    ">**Instruction:**<br>\n",
    "Use `df.diff()` to perform differencing to remove the trend. Save the return with variable name `detrend_data`\n",
    "\n",
    ">**Expected Result:**<br>\n",
    "![image](https://user-images.githubusercontent.com/59526258/117633502-ae34fa00-b1b0-11eb-8ec4-0e3788335210.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incident-crash",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cdd64939dc4404de13ef9b64934781b7",
     "grade": false,
     "grade_id": "cell-754a4d723e8fa2dd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# 1st order differencing (Detrending\n",
    "# YOUR CODE HERE\n",
    "\n",
    "detrend_data.plot()\n",
    "plt.title(\"Detrending Montly Milk Production Forecast\")\n",
    "plt.ylabel(\"Pounds per cow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simplified-johns",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADF Test\n",
    "result_adf = adfuller(detrend_data, maxlag=12)\n",
    "print_adf_result(result_adf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedicated-apartment",
   "metadata": {},
   "source": [
    "The ADF test show that the data is in stationary now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yellow-philip",
   "metadata": {},
   "source": [
    "#### <a name=\"ACFandPACFplot\">2.3.5 -  ACF and PACF plot\n",
    "After the data is stationary, use `ACF` and `PACF` to find the `p` and `q` parameters for ARIMA model.<br>\n",
    "Remind that:  <br>\n",
    "`p` is determined by `PACF`<br>\n",
    "`q` is determined by `ACF`<br>\n",
    ">**Instruction:**<br>\n",
    "Plot ACF and PACF to find the `p` and `q` parameters for ARIMA model.\n",
    "\n",
    ">**Expected Result:**<br>\n",
    "![image](https://user-images.githubusercontent.com/59526258/117634502-8c884280-b1b1-11eb-91c5-d79d91e5f983.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composed-gazette",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "66f868bf4fb1ceaea67366a5cfb4e51b",
     "grade": false,
     "grade_id": "cell-5a9a656dee169dd1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# ACF and PACF plot\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(16, 4))\n",
    "# YOUR CODE HERE\n",
    "\n",
    "ax1.set_title('ACF of differenced series')\n",
    "ax2.set_title('PACF of differenced series')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "known-syracuse",
   "metadata": {},
   "source": [
    "#### <a name=\"ARIMAmodelconfiguration\">2.3.6 - ARIMA model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "digital-unemployment",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Define ARIMA model\n",
    "arima = ARIMA(detrend_data.dropna(), order=(5,1,1)).fit()\n",
    "arima.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "starting-seeker",
   "metadata": {},
   "source": [
    "#### <a name=\"ARIMAmodelforecast\">2.3.6 - ARIMA model forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced-yield",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ARIMA model forecast\n",
    "def arima_forecast(test_data):\n",
    "    arima_forecast,_,_ = arima.forecast(len(test_data))\n",
    "    arima_forecast = pd.Series(arima_forecast, index=test_data.index)\n",
    "    return arima_forecast \n",
    "arima_forecast = arima_forecast(test_data)\n",
    "arima_forecast = pd.Series(arima_forecast)\n",
    "arima_forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indian-mixer",
   "metadata": {},
   "source": [
    "#### <a name=\"ReverseDifferencing\">2.3.7 - Reverse Differencing\n",
    "After getting the forecast data, you must reverse the differencing data because the forecast data is not on a correct scale.\n",
    "\n",
    "**Data Differencing Roadmap:**<br>\n",
    "Log Data -> Seasonal Differencing -> 1st Order Differencing \n",
    "\n",
    "**Reverse Differencing Roadmap:**<br>\n",
    "Reverse 1st Order Differencing -> Reverse Seasonal Differencing -> Inverse Log Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "destroyed-visiting",
   "metadata": {},
   "source": [
    "##### <a name=\"Reverse1storderdifferencing\">2.3.7.1 - Reverse 1st order differencing\n",
    "\n",
    "**Strategy to perform reverse 1st order differencing in time series data**<br>\n",
    "1. Stack the forecast data and `detrend_data` together and save it as `series_merge`\n",
    "2. Copy the first row of `deseasonal_data` and stack with the `series_merge`\n",
    "3. Perform cumulative sum over the `series_merge`\n",
    "\n",
    "**How to perform reverse 1st order differencing.**<br>\n",
    "Instead of subtraction, the reverse of 1st order differencing using addition. The cache columns are the places to temporary store the previous answer after performing the addition between the previous data and previous cache, as shown below figure:: \n",
    "![detrending_concept](../../picture/reverse_differencing.png)\n",
    "\n",
    "> `Cache` can be an empty list that able to store and append the latest answer \n",
    "\n",
    "In Python, there is an optional way to do this operation using `df.cumsum()`, which means perform a cumulative sum towards the data.\n",
    "For example:<br>\n",
    "![detrending_concept](../../picture/cumulative_sum.png)\n",
    "\n",
    "**How to perform reverse 1st order differencing after getting the forecast data**<br>\n",
    "First, you are required to stack the forecast data then perform the cumulative sum over the list.<br>\n",
    "For example:<br>\n",
    "![detrending_concept_forecast](../../picture/reverse_differencing_forecast.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noticed-admission",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack the forecast data and detrend_data together and save it as series_merge\n",
    "series_merge = np.hstack([detrend_data['Monthly milk production (pounds per cow)'].values,arima_forecast])\n",
    "\n",
    "# Copy the first row of deseasonal_data and stack with the series_merge\n",
    "original_data = deseasonal_data.values[0]\n",
    "\n",
    "# Perform cumulative sum over the series_merge\n",
    "trend = np.hstack([original_data,series_merge]).cumsum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dutch-greece",
   "metadata": {},
   "source": [
    "Perform sanity check to make sure the `Deseasonal Data` is identical with the `trend` data except for the forecast data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reported-cable",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(milk_data.index[12:],trend,'r',label='Forecast Data')\n",
    "plt.title(\"Reverse 1st order differencing\")\n",
    "plt.plot(deseasonal_data,label='Deseasonal Data')\n",
    "plt.legend()\n",
    "plt.ylabel(\"Pounds per cow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "printable-lighting",
   "metadata": {},
   "source": [
    "##### <a name=\"Reverseseasonaldifferencing\">2.3.7.2 - Reverse seasonal differencing\n",
    "**Strategy to perform reverse seasonal differencing in time series data**<br>\n",
    "1. Stack the first 12 of original data(data that before remove the seasonality) and `trend` data undergoing reverse 1st order differencing together. \n",
    "2. Iterate and perform summation until the end of the list.\n",
    "\n",
    "**How to perform reverse seasonal differencing?**<br>\n",
    "The concept is almost similar with the reverse 1st order differencing. The only difference is that you need to create an empty list to store the `cache`.<br> \n",
    "For example, below table shown the calculation process of reverse seasonal differencing with seasonal period=2.<br>\n",
    "![detrending_concept_forecast](../../picture/Deseasonal_Concept.png)\n",
    "    \n",
    "In Python, you can utilize the list `append()` method to store the original data and the result after performing addition with `cache`.\n",
    "![detrending_concept_forecast](../../picture/python_reverse_seasonality.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preceding-covering",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the `cache`\n",
    "inverse_seasonal = np.vstack([np.zeros((differencing_month,1)),trend.reshape(-1,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understanding-frontier",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the first 12 orignal data to list\n",
    "seasonal_data = train_data_log.values[:differencing_month].tolist()\n",
    "\n",
    "# Iterate and perform summation until the end of the list\n",
    "for i in range(differencing_month,len(trend)+differencing_month):\n",
    "    seasonal_data.append(seasonal_data[i-differencing_month] + inverse_seasonal[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "three-heather",
   "metadata": {},
   "source": [
    "Perform sanity check to make sure the `train_data_log` is same with the `seasonal_data` data except for the forecast part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "employed-imaging",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Plot the Reverse seasonal differencing for sanity check\n",
    "plt.plot(milk_data.index,seasonal_data,'r',label='Forecast Data')\n",
    "plt.plot(train_data_log,label='Logged Train Data')\n",
    "plt.title(\"Reverse seasonal differencing\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"Pounds per cow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "western-surgeon",
   "metadata": {},
   "source": [
    "The `train_data_log` is the same as the `seasonal_data` data except for the forecast part. It means that the reverse transform for 1st order differencing and seasonality differencing is correct. Do take note the data is still on the log scale, you are required to transform it back to the original scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-individual",
   "metadata": {},
   "source": [
    "#### <a name=\"InverseLogTransform\">2.3.8 - Inverse Log Transform\n",
    ">**Instruction:**<br>\n",
    "Use `np.exp` to perform exponential to logged value (`seasonal_data`) to transform the data back to original values.\n",
    "\n",
    ">**Expected Result:**<br>\n",
    "array([[589.],<br>\n",
    "       [561.],<br>\n",
    "       [640.],<br>\n",
    "       [656.],<br>\n",
    "       [727.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saved-exposure",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ca51e6143cb58033368bfdab04a8254c",
     "grade": false,
     "grade_id": "cell-f66a7e86c118f631",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Inverse Log Transform\n",
    "# YOUR CODE HERE\n",
    "\n",
    "inverse_log[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considerable-catch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Inverse log data\n",
    "plt.plot(milk_data.index,inverse_log,label ='Inverse Logged Data')\n",
    "plt.plot(milk_data.index[-len(test_data):],inverse_log[-len(test_data):],'r',label='Forecast Data')\n",
    "plt.title(\"Inverse Logged Data\")\n",
    "plt.ylabel(\"Pounds per cow\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-committee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Forecast Result\n",
    "arima_prediction = inverse_log[-len(test_data):]\n",
    "forecast_plot(forecast_data = arima_prediction,forecast_label = 'ARIMA Forecast')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "familiar-kingdom",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the result into Dataframe\n",
    "arima_forecast_result = math.sqrt(mean_squared_error(arima_prediction,test_data))\n",
    "result['ARIMA Forecast'] =  arima_forecast_result\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consistent-lesson",
   "metadata": {},
   "source": [
    "### <a name=\"SARIMAForecast\">2.4 - SARIMA Forecast\n",
    ">**Instruction:**<br>\n",
    "Use `pm.arima.auto_arima()` to perform the prediction by using SARIMA model with seasonal period, `m=12`\n",
    "\n",
    ">**Expected Result:**<br>\n",
    "Fit ARIMA: order=(2, 0, 2) seasonal_order=(1, 1, 1, 12); AIC=734.227, BIC=755.458, Fit time=0.754 seconds<br>\n",
    "Fit ARIMA: order=(0, 0, 0) seasonal_order=(0, 1, 0, 12); AIC=848.245, BIC=853.553, Fit time=0.009 seconds<br>\n",
    "Fit ARIMA: order=(1, 0, 0) seasonal_order=(1, 1, 0, 12); AIC=741.855, BIC=752.471, Fit time=0.154 seconds<br>\n",
    ".<br>\n",
    ".<br>\n",
    ".<br>\n",
    "Fit ARIMA: order=(1, 0, 0) seasonal_order=(0, 1, 1, 12); AIC=733.075, BIC=743.691, Fit time=0.120 seconds<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mineral-fishing",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dc03f3017337327dd8d08213092f8e40",
     "grade": false,
     "grade_id": "cell-dc162ab5a6279543",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import six\n",
    "import sys\n",
    "sys.modules['sklearn.externals.six'] = six\n",
    "import joblib\n",
    "sys.modules['sklearn.externals.joblib'] = joblib\n",
    "import pmdarima as pm\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-lyric",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SARIMA Summary\n",
    "auto_arima.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "published-clone",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SARIMA model prediction\n",
    "auto_arima_forecast = auto_arima.predict(len(test_data))\n",
    "auto_arima_forecast_series = pd.Series(auto_arima_forecast, index=test_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "downtown-scenario",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the forecast data\n",
    "forecast_plot(forecast_data = auto_arima_forecast_series, forecast_label = 'SARIMA Forecast') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lyric-bermuda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the result into Dataframe\n",
    "sarima_result = math.sqrt(mean_squared_error(test_data,auto_arima_forecast_series))\n",
    "result['SARIMA Forecast'] =  sarima_result\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exotic-retail",
   "metadata": {},
   "source": [
    "# <a name=\"TimeSeriesModelingwithDeepLearningMethod(MLP)\">3. Time Series Modeling with Deep Learning Method (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-salad",
   "metadata": {},
   "source": [
    "### <a name=\"Hyperparamter\">3.1 - Hyperparamter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crude-demographic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparamter\n",
    "window_size = 3\n",
    "n_epoch = 500\n",
    "batch_size = 5 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handed-excess",
   "metadata": {},
   "source": [
    "### <a name=\"DataScaling\">3.2 - Data Scaling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "built-scratch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Scaling\n",
    "scaler = StandardScaler().fit(train_data)\n",
    "train_data_scale = scaler.transform(train_data)\n",
    "test_data_scale = scaler.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupied-threat",
   "metadata": {},
   "source": [
    "### <a name=\"WindowSliding\">3.3 - Window Sliding \n",
    "#### Optional 1 - Use the previous sliding window function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "present-upgrade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window Sliding Function\n",
    "def sliding_window(univariate_data,window_size):\n",
    "    x,y = list(),list()\n",
    "    for i in range(len(univariate_data)):\n",
    "        end_ix = i + window_size\n",
    "        if end_ix > len(univariate_data)-1:\n",
    "            break\n",
    "        seq_x, seq_y = univariate_data[i:end_ix], univariate_data[end_ix]\n",
    "        x.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(x),np.array(y)\n",
    "\n",
    "train_feature , train_label = sliding_window(train_data_scale,window_size)\n",
    "test_feature , test_label = sliding_window(test_data_scale,window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "productive-assist",
   "metadata": {},
   "source": [
    "#### Optional 2 - Use helper function from data_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlled-reducing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Function from previous exercise\n",
    "import data_module\n",
    "train_feature , train_label = data_module.univariate_single_step(train_data_scale,window_size)\n",
    "test_feature , test_label = data_module.univariate_single_step(test_data_scale,window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frequent-width",
   "metadata": {},
   "source": [
    "### <a name=\"DataIterator\">3.4 - Data Iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opening-guest",
   "metadata": {},
   "source": [
    "####  Optional 1 - Using Pytorch Custom Dataset Method\n",
    ">**Instruction:**<br>\n",
    "Create Data Iterator using Pytorch Custom Dataset Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "popular-census",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "51231f4faedfc9407263c5e947ad666a",
     "grade": false,
     "grade_id": "cell-dd23cf493beb84f9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Custom_Dataset(Dataset):\n",
    "    # YOUR CODE HERE\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accomplished-infrared",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Custom_Dataset(train_feature,train_label)\n",
    "test_dataset = Custom_Dataset(test_feature,test_label)\n",
    "train_iterator = DataLoader(train_dataset,batch_size,shuffle = False)\n",
    "test_iterator = DataLoader(test_dataset,batch_size,shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-learning",
   "metadata": {},
   "source": [
    "#### Optional 2 - TensorDataset\n",
    ">**Instruction:**<br>\n",
    "Create Data Iterator using `TensorDataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driven-consortium",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "366638d53d1a61b1037e9e7889470044",
     "grade": false,
     "grade_id": "cell-332b90cbc0eb1f25",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "trainX = torch.from_numpy(train_feature).type(torch.Tensor)\n",
    "trainY = torch.from_numpy(train_label).type(torch.Tensor)\n",
    "testX = torch.from_numpy(test_feature).type(torch.Tensor)\n",
    "testY = torch.from_numpy(test_label).type(torch.Tensor)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "train_iterator = DataLoader(train_dataset,batch_size,shuffle = False)\n",
    "test_iterator = DataLoader(test_dataset,batch_size,shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressed-struggle",
   "metadata": {},
   "source": [
    "### <a name=\"Multilayerperceptron(MLP)configuration\">3.5 - Multilayer perceptron (MLP) configuration\n",
    ">**Instruction:**<br>\n",
    "Create the MLP configuration based on the **Expected Result:**<br>\n",
    "\n",
    ">**Expected Result:**<br>\n",
    "![image](https://user-images.githubusercontent.com/59526258/118422293-83d5c600-b6f5-11eb-842e-300b63950228.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flush-mount",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self,input_size,output_size):\n",
    "        super(MLP,self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.input_layer = nn.Linear(input_size,10)\n",
    "        self.hidden_layer = nn.Linear(10,5)\n",
    "        self.output_layer = nn.Linear(5,output_size)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = x.view(-1,input_size)\n",
    "        out = F.relu(self.input_layer(x))\n",
    "        out = F.relu(self.hidden_layer(out))\n",
    "        out = self.output_layer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "further-fortune",
   "metadata": {},
   "source": [
    "### <a name=\"InputModel\">3.6 - Input Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nearby-canada",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "input_size = window_size\n",
    "output_size = 1\n",
    "model = MLP(input_size, output_size)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr= 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominant-theater",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xavier weight intialization\n",
    "torch.manual_seed(123)\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "\n",
    "        \n",
    "model.apply(weights_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "original-entity",
   "metadata": {},
   "source": [
    "### <a name=\"ModelSummary\">3.7 - Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eastern-blackberry",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# pip install this package to view the summary of model  \n",
    "# used pip install due to it does not have conda version\n",
    "# %%capture suppress information of torchsummaryX installation\n",
    "!pip install torchsummaryX\n",
    "from torchsummaryX import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "written-collins",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.zeros((batch_size,window_size),dtype=torch.float) # batch size,seq_dimension\n",
    "print(summary(model,inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surface-boating",
   "metadata": {},
   "source": [
    "### <a name=\"Training\">3.8 - Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cordless-scheme",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3cf2c289ec2248d25d2e9d2e1f55b98d",
     "grade": false,
     "grade_id": "cell-3e2f2017c400a0de",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def training(num_epochs,train_iter,test_iter,optimizer,loss_fn,model):\n",
    "    # Create a list of zero value to store the averaged value\n",
    "    train_loss = np.zeros(num_epochs)\n",
    "    val_loss = np.zeros(num_epochs)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return train_loss,val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functional-shower",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss,val_loss = training(n_epoch,train_iterator,test_iterator,optimizer,loss_fn,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-berlin",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_epoch):\n",
    "    print(f\"Epoch: {i}, train loss: {train_loss[i]} ,test loss: {val_loss[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caring-saudi",
   "metadata": {},
   "source": [
    "### <a name=\"Validation\">3.9 - Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separate-birthday",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    train_prediction = model(trainX)\n",
    "    test_prediction = model(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chubby-assistant",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse Scaling\n",
    "train_label_rescale = scaler.inverse_transform(train_label)\n",
    "test_label_rescale = scaler.inverse_transform(test_label)\n",
    "train_prediction_rescale = scaler.inverse_transform(train_prediction)\n",
    "test_prediction_rescale = scaler.inverse_transform(test_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powered-point",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test Data\\t\\t\\tForecast Data\")\n",
    "for i in range(len(test_label_rescale )):\n",
    "    print(f\"{test_label_rescale[i]}\\t\\t{test_prediction_rescale[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alike-internet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Forecast Data\n",
    "forecast_plot(forecast_data = test_label_rescale, \n",
    "              forecast_label = 'MLP Forecast',\n",
    "              test_time=test_time[window_size:],\n",
    "              test_data=test_prediction_rescale) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connected-array",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_train_result = math.sqrt(mean_squared_error(train_label_rescale,train_prediction_rescale))\n",
    "mlp_forecast_result = math.sqrt(mean_squared_error(test_label_rescale,test_prediction_rescale))\n",
    "print('Train Score: %.2f RMSE' % (mlp_train_result))\n",
    "print('Test Score: %.2f RMSE' % (mlp_forecast_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fancy-peeing",
   "metadata": {},
   "outputs": [],
   "source": [
    "result['MLP Forecast'] =  mlp_forecast_result\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partial-sending",
   "metadata": {},
   "source": [
    "In conclusion, MLP model gives the lowest RMSE which is appropriate to make it as our forecast model for future unseen data. The second option goes to SARIMA model which has a slightly higher RMSE than MLP model, but have a faster runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demographic-chassis",
   "metadata": {},
   "source": [
    "## <a name=\"Summary\">Summary\n",
    "From this tutorial, you should have learned:\n",
    "\n",
    "1. Understand the time series forecasting flow.\n",
    "2. Compare and select the optimal model as forecasting model base on the model performance. \n",
    "\n",
    "This tutorial only covers MLP in the Deep Learning section. You may include the LSTM and CNN deep learning model on your own and observe the performance between them.<br>\n",
    "    \n",
    "Congratulations, that concludes this lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fuzzy-national",
   "metadata": {},
   "source": [
    "## <a name=\"Reference\">Reference\n",
    "1. [Deep Learning for Time Series Forecasting (Predict the Future with MLPs,CNNs and LSTMs in Python) , Jason Brownlee](https://machinelearningmastery.com/deep-learning-for-time-series-forecasting/)\n",
    "2. [Time-series Forecasting Flow](https://towardsdatascience.com/time-series-forecasting-flow-2e49740664de)"
   ]
  }
 ],
 "metadata": {
  "gist": {
   "data": {
    "description": "Desktop/time series labs/time-series-labs/notebook/Solution/03 - Recap Exercise .ipynb",
    "public": false
   },
   "id": ""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
