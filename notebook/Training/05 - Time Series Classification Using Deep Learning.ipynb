{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "![logo](../../picture/license_header_logo.png)\n",
    "> **Copyright &copy; 2020 - 2021 CertifAI Sdn. Bhd.**<br>\n",
    " <br>\n",
    "This program and the accompanying materials are made available under the\n",
    "terms of the [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0). <br>\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n",
    "WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n",
    "License for the specific language governing permissions and limitations\n",
    "under the License. <br>\n",
    "<br>**SPDX-License-Identifier: Apache-2.0**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 - Time Series Classification Using Deep Learning\n",
    "\n",
    "Revised by: [Kian Yang Lee](https://github.com/KianYang-Lee) - kianyang.lee@certifai.ai\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Description\n",
    "\n",
    "This tutorial will showcase how to perform time series classifiation using long short-term memory network, a variation of deep learning model. PyTorch deep learning framework is used for this notebook. An exercise section is attached for you to practice and hone your skills. Do make good use of it.\n",
    "\n",
    "By the end of this tutorial, you will be able to:\n",
    "\n",
    "1. Prepare dataset to be feed into deep learning model\n",
    "2. Build and apply LSTM model \n",
    "3. Perform classification tasks on time series data\n",
    "\n",
    "## Notebook Outline\n",
    "Below is the outline for this tutorial:\n",
    "1. [Notebook Configurations](#configuration)\n",
    "2. [Dataset](#dataset)\n",
    "3. [Basic Analytics](#analytics)\n",
    "4. [Model Development](#model-dev) \n",
    "5. [Evaluation](#evaluation)\n",
    "6. [Exercise](#exercise)\n",
    "7. [Reference](#reference)\n",
    "8. [Bonus](#bonus)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"configuration\">Notebook Configurations</a>\n",
    "Following are the modules that will be used for this tutorial. This notebook will heavily use `PyTorch` as they provide great APIs when for building deep learning models. You can find out more about them [here](https://pytorch.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x25fca066730>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, classification_report\n",
    "import sys\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, IterableDataset\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# reproducibility\n",
    "seed = 38\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"dataset\">Dataset</a>\n",
    "The dataset to be used is the NASA Turbofan dataset. You can read more about the dataset [here](https://www.kaggle.com/c/predictive-maintenance)\n",
    "\n",
    "The dataset consists of different multivariate time-series. These different time-series refer to different engines. The sampling of the time series is 1 point per engine cycle.\n",
    "\n",
    "Predictive maintenance techniques are designed to help determine the condition of in-service equipment in order to\n",
    "estimate when maintenance should be performed. Predictive maintenance can be modeled in several ways,\n",
    "1. Predict the Remaining Useful Life (RUL), or Time to Failure (TTF)\n",
    "2. Predict if the asset will fail by given a certain time frame\n",
    "3. Predict critical level of the asset by give a certain time frame\n",
    "\n",
    "For this example, we will be using the 2nd modeling strategy. We are going to predict weather the asset is going to fail. \n",
    "\n",
    "The target variable to be used is \"Label1\". It consist of 0 and 1. 0 means the assets is working fine and 1 means it require maintenance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing dataset\n",
    "df_train = pd.read_csv(\"../../datasets/predictive_maintenance/train.csv\")\n",
    "df_test = pd.read_csv(\"../../datasets/predictive_maintenance/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"analytics\">Basic Analytics</a>\n",
    "Let's do some basic analytics and see what nugget can we find out of the training dataset! Let first find out the number of rows and columns of the training dataset.\n",
    "\n",
    "**Note: Since we are not supposed to know any information about the test dataset, we will not perform any analytics on it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cycle</th>\n",
       "      <th>setting1</th>\n",
       "      <th>setting2</th>\n",
       "      <th>setting3</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "      <th>s7</th>\n",
       "      <th>s8</th>\n",
       "      <th>s9</th>\n",
       "      <th>s10</th>\n",
       "      <th>s11</th>\n",
       "      <th>s12</th>\n",
       "      <th>s13</th>\n",
       "      <th>s14</th>\n",
       "      <th>s15</th>\n",
       "      <th>s16</th>\n",
       "      <th>s17</th>\n",
       "      <th>s18</th>\n",
       "      <th>s19</th>\n",
       "      <th>s20</th>\n",
       "      <th>s21</th>\n",
       "      <th>cycle_norm</th>\n",
       "      <th>RUL</th>\n",
       "      <th>label1</th>\n",
       "      <th>label2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.459770</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.183735</td>\n",
       "      <td>0.406802</td>\n",
       "      <td>0.309757</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.726248</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.109755</td>\n",
       "      <td>0</td>\n",
       "      <td>0.369048</td>\n",
       "      <td>0.633262</td>\n",
       "      <td>0.205882</td>\n",
       "      <td>0.199608</td>\n",
       "      <td>0.363986</td>\n",
       "      <td>0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.713178</td>\n",
       "      <td>0.724662</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>191</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.609195</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.283133</td>\n",
       "      <td>0.453019</td>\n",
       "      <td>0.352633</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.628019</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>0.100242</td>\n",
       "      <td>0</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.765458</td>\n",
       "      <td>0.279412</td>\n",
       "      <td>0.162813</td>\n",
       "      <td>0.411312</td>\n",
       "      <td>0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.731014</td>\n",
       "      <td>0.00277</td>\n",
       "      <td>190</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.252874</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.343373</td>\n",
       "      <td>0.369523</td>\n",
       "      <td>0.370527</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.710145</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.140043</td>\n",
       "      <td>0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.795309</td>\n",
       "      <td>0.220588</td>\n",
       "      <td>0.171793</td>\n",
       "      <td>0.357445</td>\n",
       "      <td>0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.627907</td>\n",
       "      <td>0.621375</td>\n",
       "      <td>0.00554</td>\n",
       "      <td>189</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.540230</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.343373</td>\n",
       "      <td>0.256159</td>\n",
       "      <td>0.331195</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.124518</td>\n",
       "      <td>0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.889126</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.174889</td>\n",
       "      <td>0.166603</td>\n",
       "      <td>0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573643</td>\n",
       "      <td>0.662386</td>\n",
       "      <td>0.00831</td>\n",
       "      <td>188</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.390805</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.349398</td>\n",
       "      <td>0.257467</td>\n",
       "      <td>0.404625</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.668277</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.149960</td>\n",
       "      <td>0</td>\n",
       "      <td>0.255952</td>\n",
       "      <td>0.746269</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.174734</td>\n",
       "      <td>0.402078</td>\n",
       "      <td>0</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.589147</td>\n",
       "      <td>0.704502</td>\n",
       "      <td>0.01108</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  cycle  setting1  setting2  setting3  s1        s2        s3        s4  \\\n",
       "0   1      1  0.459770  0.166667         0   0  0.183735  0.406802  0.309757   \n",
       "1   1      2  0.609195  0.250000         0   0  0.283133  0.453019  0.352633   \n",
       "2   1      3  0.252874  0.750000         0   0  0.343373  0.369523  0.370527   \n",
       "3   1      4  0.540230  0.500000         0   0  0.343373  0.256159  0.331195   \n",
       "4   1      5  0.390805  0.333333         0   0  0.349398  0.257467  0.404625   \n",
       "\n",
       "   s5  s6        s7        s8        s9  s10       s11       s12       s13  \\\n",
       "0   0   1  0.726248  0.242424  0.109755    0  0.369048  0.633262  0.205882   \n",
       "1   0   1  0.628019  0.212121  0.100242    0  0.380952  0.765458  0.279412   \n",
       "2   0   1  0.710145  0.272727  0.140043    0  0.250000  0.795309  0.220588   \n",
       "3   0   1  0.740741  0.318182  0.124518    0  0.166667  0.889126  0.294118   \n",
       "4   0   1  0.668277  0.242424  0.149960    0  0.255952  0.746269  0.235294   \n",
       "\n",
       "        s14       s15  s16       s17  s18  s19       s20       s21  \\\n",
       "0  0.199608  0.363986    0  0.333333    0    0  0.713178  0.724662   \n",
       "1  0.162813  0.411312    0  0.333333    0    0  0.666667  0.731014   \n",
       "2  0.171793  0.357445    0  0.166667    0    0  0.627907  0.621375   \n",
       "3  0.174889  0.166603    0  0.333333    0    0  0.573643  0.662386   \n",
       "4  0.174734  0.402078    0  0.416667    0    0  0.589147  0.704502   \n",
       "\n",
       "   cycle_norm  RUL  label1  label2  \n",
       "0     0.00000  191       0       0  \n",
       "1     0.00277  190       0       0  \n",
       "2     0.00554  189       0       0  \n",
       "3     0.00831  188       0       0  \n",
       "4     0.01108  187       0       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect first 5 rows of training data\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important note is that the target variable needs to have balanced distribution for us to decide which metrics that is suitable to be used for model evaluation. So, we will check for distribution of target variable first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes in train dataset: Counter({0: 17531, 1: 3100})\n"
     ]
    }
   ],
   "source": [
    "# compute class distribution\n",
    "class_distribution = Counter(df_train[\"label1\"])\n",
    "\n",
    "print('Classes in train dataset:', class_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above class distribution shwows that the dataset has imbalance class, with ratio of around 3.5 : 1. Next, we will inspect are there any missing values in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id            0\n",
       "cycle         0\n",
       "setting1      0\n",
       "setting2      0\n",
       "setting3      0\n",
       "s1            0\n",
       "s2            0\n",
       "s3            0\n",
       "s4            0\n",
       "s5            0\n",
       "s6            0\n",
       "s7            0\n",
       "s8            0\n",
       "s9            0\n",
       "s10           0\n",
       "s11           0\n",
       "s12           0\n",
       "s13           0\n",
       "s14           0\n",
       "s15           0\n",
       "s16           0\n",
       "s17           0\n",
       "s18           0\n",
       "s19           0\n",
       "s20           0\n",
       "s21           0\n",
       "cycle_norm    0\n",
       "RUL           0\n",
       "label1        0\n",
       "label2        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect missing values\n",
    "df_train.isna().sum() # shows that there are no missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appears to be no missing values based on the above display. Currently, the dataset contains a number of features. However, not all features are useful for model building. We will first remove some unnecessary features before we proceed with model building. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original dataset has 30 columns.\n",
      "After removing unnecessary columns, the dataset now has 21 columns.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cycle</th>\n",
       "      <th>setting1</th>\n",
       "      <th>setting2</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s6</th>\n",
       "      <th>s7</th>\n",
       "      <th>s8</th>\n",
       "      <th>s9</th>\n",
       "      <th>s11</th>\n",
       "      <th>s12</th>\n",
       "      <th>s13</th>\n",
       "      <th>s14</th>\n",
       "      <th>s15</th>\n",
       "      <th>s17</th>\n",
       "      <th>s20</th>\n",
       "      <th>s21</th>\n",
       "      <th>cycle_norm</th>\n",
       "      <th>label1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.459770</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.183735</td>\n",
       "      <td>0.406802</td>\n",
       "      <td>0.309757</td>\n",
       "      <td>1</td>\n",
       "      <td>0.726248</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.109755</td>\n",
       "      <td>0.369048</td>\n",
       "      <td>0.633262</td>\n",
       "      <td>0.205882</td>\n",
       "      <td>0.199608</td>\n",
       "      <td>0.363986</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.713178</td>\n",
       "      <td>0.724662</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.609195</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.283133</td>\n",
       "      <td>0.453019</td>\n",
       "      <td>0.352633</td>\n",
       "      <td>1</td>\n",
       "      <td>0.628019</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>0.100242</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.765458</td>\n",
       "      <td>0.279412</td>\n",
       "      <td>0.162813</td>\n",
       "      <td>0.411312</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.731014</td>\n",
       "      <td>0.00277</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.252874</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.343373</td>\n",
       "      <td>0.369523</td>\n",
       "      <td>0.370527</td>\n",
       "      <td>1</td>\n",
       "      <td>0.710145</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.140043</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.795309</td>\n",
       "      <td>0.220588</td>\n",
       "      <td>0.171793</td>\n",
       "      <td>0.357445</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.627907</td>\n",
       "      <td>0.621375</td>\n",
       "      <td>0.00554</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.540230</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.343373</td>\n",
       "      <td>0.256159</td>\n",
       "      <td>0.331195</td>\n",
       "      <td>1</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.124518</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.889126</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.174889</td>\n",
       "      <td>0.166603</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.573643</td>\n",
       "      <td>0.662386</td>\n",
       "      <td>0.00831</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.390805</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.349398</td>\n",
       "      <td>0.257467</td>\n",
       "      <td>0.404625</td>\n",
       "      <td>1</td>\n",
       "      <td>0.668277</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.149960</td>\n",
       "      <td>0.255952</td>\n",
       "      <td>0.746269</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.174734</td>\n",
       "      <td>0.402078</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.589147</td>\n",
       "      <td>0.704502</td>\n",
       "      <td>0.01108</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  cycle  setting1  setting2        s2        s3        s4  s6        s7  \\\n",
       "0   1      1  0.459770  0.166667  0.183735  0.406802  0.309757   1  0.726248   \n",
       "1   1      2  0.609195  0.250000  0.283133  0.453019  0.352633   1  0.628019   \n",
       "2   1      3  0.252874  0.750000  0.343373  0.369523  0.370527   1  0.710145   \n",
       "3   1      4  0.540230  0.500000  0.343373  0.256159  0.331195   1  0.740741   \n",
       "4   1      5  0.390805  0.333333  0.349398  0.257467  0.404625   1  0.668277   \n",
       "\n",
       "         s8        s9       s11       s12       s13       s14       s15  \\\n",
       "0  0.242424  0.109755  0.369048  0.633262  0.205882  0.199608  0.363986   \n",
       "1  0.212121  0.100242  0.380952  0.765458  0.279412  0.162813  0.411312   \n",
       "2  0.272727  0.140043  0.250000  0.795309  0.220588  0.171793  0.357445   \n",
       "3  0.318182  0.124518  0.166667  0.889126  0.294118  0.174889  0.166603   \n",
       "4  0.242424  0.149960  0.255952  0.746269  0.235294  0.174734  0.402078   \n",
       "\n",
       "        s17       s20       s21  cycle_norm  label1  \n",
       "0  0.333333  0.713178  0.724662     0.00000       0  \n",
       "1  0.333333  0.666667  0.731014     0.00277       0  \n",
       "2  0.166667  0.627907  0.621375     0.00554       0  \n",
       "3  0.333333  0.573643  0.662386     0.00831       0  \n",
       "4  0.416667  0.589147  0.704502     0.01108       0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing the unnecessary columns\n",
    "drop_columns_list = [\"setting3\", \"s1\", \"s5\", \"s10\", \"s16\", \"s18\", \"s19\", \"RUL\", \"label2\"]\n",
    "df_train_cleaned = df_train.drop(drop_columns_list, axis=1)\n",
    "df_test_cleaned = df_test.drop(drop_columns_list, axis=1)\n",
    "\n",
    "print(f\"The original dataset has {df_train.shape[1]} columns.\")\n",
    "print(f\"After removing unnecessary columns, the dataset now has {df_train_cleaned.shape[1]} columns.\")\n",
    "\n",
    "df_train_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are also interested in the data type of each column when we are building a model. We will analyze the data type of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id              int64\n",
       "cycle           int64\n",
       "setting1      float64\n",
       "setting2      float64\n",
       "s2            float64\n",
       "s3            float64\n",
       "s4            float64\n",
       "s6              int64\n",
       "s7            float64\n",
       "s8            float64\n",
       "s9            float64\n",
       "s11           float64\n",
       "s12           float64\n",
       "s13           float64\n",
       "s14           float64\n",
       "s15           float64\n",
       "s17           float64\n",
       "s20           float64\n",
       "s21           float64\n",
       "cycle_norm    float64\n",
       "label1          int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for the data types\n",
    "df_train_cleaned.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take note of a few things here. First, the 'ID' column should be indicative of which machine that the data is collected, while the 'Cycle' column indicates the time step that the measurement is taken from. The rest all of the variables have values of more or less the same scale, that is within 0 and 1. This could indicate that someone has already performed Min-Max Normalization for us. Nonetheless, we will also perform it later just to be on the safe side. Before we proceed though, we will first separate out the features and labels for later usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape for y of train dataset:  (20631,)\n",
      "Shape for x of train dataset:  (20631, 20)\n",
      "Shape for y of test dataset:  (13096,)\n",
      "Shape for x of test dataset:  (13096, 20)\n"
     ]
    }
   ],
   "source": [
    "# separate out features and target variable\n",
    "y_train = df_train_cleaned['label1']\n",
    "X_train = df_train_cleaned.drop('label1', axis=1)\n",
    "y_test = df_test_cleaned['label1']\n",
    "X_test = df_test_cleaned.drop('label1', axis=1)\n",
    "\n",
    "print('Shape for y of train dataset: ', y_train.shape)\n",
    "print('Shape for x of train dataset: ', X_train.shape)\n",
    "print('Shape for y of test dataset: ', y_test.shape)\n",
    "print('Shape for x of test dataset: ', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have separated out the features and the label. We also see that we have 20,631 samples for our training data and 13,096 samples for our test data. Next, we will perform feature scaling using min-max normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data pre-processing : min-max normalization\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"model-dev\">Model Development</a>\n",
    "After performing some basic analytics on the dataset at hand, we will start to build deep learning model to classify time series for this dataset. We will be guiding you through the process of developing a LSTM model using `PyTorch`.\n",
    "\n",
    "LSTM is a variant of recurrent neural network (RNN). RNNs have loops in the network that, theoretically, allows for past information to persist. This feature helps to overcome one of the shortcomings of a traditional neural network, where it is ineffective in modeling sequential data.\n",
    "\n",
    "RNNs have been applied successfully in many tasks involving sequential data, such as language modeling, machine translation, etc. However, there are certain downsides to a vanilla RNN. RNN is notoriously known for its inability to handle long-term dependencies, and result in vanishing gradient issue. This is not a good news for a network that was initially designed to for such task.\n",
    "\n",
    "Therefore, LSTMs is popularizeed over RNNs in order to more effectively model sequential data. Its combination of forget gate, input gate, memory gate and hidden cell along with other model architecture details enable it to more effectively handle long-term dependencies over RNN. We will be guiding you on how to build a LSTM model in this tutorial.\n",
    "\n",
    "If you are interested to find out more about LSTM, Colah provides a great explanation of LSTM, which can be found [here](https://colah.github.io/posts/2015-08-Understanding-LSTMs/).\n",
    "\n",
    "Before we start building the model though, we have to first prepare our data into the format that can be fed into LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# convert dtype to np.ndarray and reshaping it to 2D array\n",
    "y_train = y_train.to_numpy().reshape(-1, 1)\n",
    "y_test = y_test.to_numpy().reshape(-1, 1)\n",
    "\n",
    "print(type(X_train_scaled))\n",
    "print(type(X_test_scaled))\n",
    "print(type(y_train))\n",
    "print(type(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should have `np.ndarray` as the data type for both features and labels now. And indeed the above cell output confirms this. Recall that we need to prepare our dataset into format that is suitable for LSTM to be trained on. We will write a helper function to perform this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples for X train: 20602\n",
      "Total samples for y train: 20602\n",
      "Total samples for X test: 13067\n",
      "Total samples for y test: 13067\n"
     ]
    }
   ],
   "source": [
    "def sequencing_data(x_data, y_data, sequence_length):\n",
    "    \"\"\"\n",
    "    Helper function to sample sub-sequence of training data.\n",
    "    Input data must be numpy.\n",
    "    \"\"\"\n",
    "    x, y = [], []\n",
    "\n",
    "    # Fill the batch with random sequences of data\n",
    "    for i in range(x_data.shape[0] - sequence_length):\n",
    "\n",
    "        # copy the sequences of data starting at this index\n",
    "        x.append(x_data[i:i + sequence_length + 1])\n",
    "        y.append(y_data[i + sequence_length])\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "# converting data shape and sequence\n",
    "X_sequence_train, y_sequence_train = sequencing_data(X_train_scaled, y_train, 29)\n",
    "X_sequence_test, y_sequence_test = sequencing_data(X_test_scaled, y_test, 29)\n",
    "\n",
    "# sanity check \n",
    "print(\"Total samples for X train: \" + str(len(X_sequence_train)))\n",
    "print(\"Total samples for y train: \" + str(len(y_sequence_train)))\n",
    "print(\"Total samples for X test: \" + str(len(X_sequence_test)))\n",
    "print(\"Total samples for y test: \" + str(len(y_sequence_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us use write a customized class that inherits `Dataset` class from `PyTorch` to transform our dataset into `Dataset` class. This then allows us to make use of `DataLoader` method of `PyTorch` to load data batch by batch into the model during training process. This is also the typical way of preparing input data in `PyTorch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a customized Dataset class\n",
    "class MaintenanceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Convert input data into torch FloatTensor. \n",
    "    Inherit Dataset class. Return length of instance when len method is called and return specific sample\n",
    "    of data when indexed.\n",
    "    \"\"\"\n",
    "    def __init__(self, x, y):\n",
    "        self.x = torch.FloatTensor(x)\n",
    "        self.y = torch.FloatTensor(y)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.x.size(0)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also do a sanity check after using Dataset class of PyTorch to ensure that we have prepared our data into the right format before we feed them into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20602\n",
      "2\n",
      "30\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "# creating train and test datasets\n",
    "train_ds = MaintenanceDataset(X_sequence_train, y_sequence_train)\n",
    "test_ds = MaintenanceDataset(X_sequence_test, y_sequence_test)\n",
    "\n",
    "# let us print out the first two rows of train dataset and check the shape of dataset too\n",
    "print(len(train_ds))          # print out number of samples\n",
    "print(len(train_ds[0]))       # print out the tuple of first sample\n",
    "print(len(train_ds[0][0]))    # print out number of features/channels\n",
    "print(len(train_ds[0][0][0])) # print out number of sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, after preparing it into the right format, we will use DataLoader to transform our data into iterable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate DataLoader instance\n",
    "batch_size = 200\n",
    "train_loader = DataLoader(dataset=train_ds,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=False)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_ds,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=False)\n",
    "\n",
    "dataloaders = {'train': train_loader, 'test': test_loader}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our input data are finally ready. Nevertheless, we still need to define the architecture of our model. The codes below show how we can define a LSTM model for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model architecture is: \n",
      "\n",
      "\n",
      "LSTM(\n",
      "  (lstm): LSTM(20, 128, num_layers=2, batch_first=True)\n",
      "  (linear): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# defining model architecture\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTM, self).__init__() \n",
    "        self.hidden_size = hidden_size \n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True) # LSTM receives input in the form of (batch_size, sequence_length, n_features)\n",
    "        self.linear = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size) # initiate zeros tensor with (num_layers, batch_size, hidden_size)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = out[:, -1, :] # we just using last time step to do prediction\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "num_layers = 2       # stack 2 RNN together\n",
    "num_classes = 1\n",
    "input_size = 20      # since one row has 20 features, we are reading one row at a time\n",
    "hidden_size = 128 \n",
    "    \n",
    "model = LSTM(input_size, hidden_size, num_layers, num_classes)\n",
    "print(\"Model architecture is: \\n\\n\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will have to configure our hyperparameters before we proceed to model training. Do note that the values shown here are not the only options that you can use. Hyperparameter tuning is a trial-and-error process. You will have to find out for yourself what kind of hyperparameter works best for different datasets and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyperparameters for model\n",
    "epochs = 5\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.BCEWithLogitsLoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to write code to perform model training and validation. For each epoch, we will train our model on training set and validate it on test set. We will have to set our model to `train` model during model training for backpropagation to occur, and set our model to `eval` mode to disable model parameters from updating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "TRAIN Loss: 0.31239788825448744\n",
      "TEST Loss: 0.07645066037148025\n",
      "Epoch 2/5\n",
      "TRAIN Loss: 0.15999575623364173\n",
      "TEST Loss: 0.04655626958642781\n",
      "Epoch 3/5\n",
      "TRAIN Loss: 0.1365068609500878\n",
      "TEST Loss: 0.04325956883612546\n",
      "Epoch 4/5\n",
      "TRAIN Loss: 0.11921028146176153\n",
      "TEST Loss: 0.03838639199649846\n",
      "Epoch 5/5\n",
      "TRAIN Loss: 0.10986944579585964\n",
      "TEST Loss: 0.04330266736282601\n"
     ]
    }
   ],
   "source": [
    "loss_score = {'train': [], 'test': []}\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch+1}/{epochs}')\n",
    "    for phase in ['train', 'test']:\n",
    "        if phase == 'train':\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "\n",
    "        loss_cumsum = 0.0\n",
    "        total_iterations = 0.0\n",
    "\n",
    "        for i, (X, y) in enumerate(dataloaders[phase]):\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                out = model(X)\n",
    "                loss = criterion(out, y)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            loss_cumsum += loss.item() * out.size(0)\n",
    "            total_iterations += out.size(0)\n",
    "\n",
    "        epoch_loss = loss_cumsum / total_iterations\n",
    "        print(f'{phase.upper()} Loss: {epoch_loss}')\n",
    "        loss_score[phase].append(epoch_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have stored the loss score during model training and validation for each epoch. We will plot a line chart of loss score (y-axis) against number of epochs (x-axis). This plot will help us to check whether the model is converging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot loss score for training and validation loop\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(14, 7)\n",
    "ax.set_title(\"Loss Score against Epoch\")\n",
    "ax.grid(b=True)\n",
    "ax.set_xlabel(\"Epoch Number\")\n",
    "ax.set_ylabel(\"Loss Score\")\n",
    "ax.plot(loss_score['train'], color='goldenrod', label='Training Loss')\n",
    "ax.plot(loss_score['test'], color='green', label='Test Loss')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the loss score plot above that the model is converging as there are no sudden upward spikes at the last few epochs. Our model is ready to be tested to check for its performance on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"evaluation\">Evaluation</a>\n",
    "Since this is a classification task, we will be using the following metrics for model evaluation:\n",
    "* accuracy\n",
    "* ROC score\n",
    "* precision\n",
    "* recall\n",
    "* f1-score\n",
    "\n",
    "We will first examine the performance of the model on both the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL PERFORMANCE ON TRAINING DATASET\n",
      "\n",
      "\n",
      "ROC AUC score: 0.988\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.97      0.97     17502\n",
      "         1.0       0.85      0.86      0.86      3100\n",
      "\n",
      "    accuracy                           0.96     20602\n",
      "   macro avg       0.91      0.92      0.92     20602\n",
      "weighted avg       0.96      0.96      0.96     20602\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using threshold to determine which prediction belongs to class 1 or 0\n",
    "threshold = 0.5\n",
    "\n",
    "# compute prediction results in the form of numpy ndarray\n",
    "model.eval()\n",
    "true_list, preds_list = [], []\n",
    "for x_test, y_test in DataLoader(dataset=train_ds, batch_size=1, shuffle=False):\n",
    "    with torch.no_grad():\n",
    "        true_list.append(y_test.detach().numpy())\n",
    "        preds = model(x_test)\n",
    "        preds = torch.sigmoid(preds)\n",
    "        preds_list.append(preds.detach().numpy())\n",
    "\n",
    "train_true_np, train_preds_np = np.concatenate(true_list), np.concatenate(preds_list)\n",
    "\n",
    "# compute evaluation metrics including ROC AUC score, accuracy score, f1-score, precision and recall\n",
    "print(\"MODEL PERFORMANCE ON TRAINING DATASET\")\n",
    "print(f'\\n\\nROC AUC score: {round(roc_auc_score(train_true_np, train_preds_np), 3)}')\n",
    "print(classification_report(y_true=train_true_np, y_pred=(train_preds_np > threshold).astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL PERFORMANCE ON TEST DATASET\n",
      "\n",
      "\n",
      "ROC AUC score: 0.989\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.99      0.99     12735\n",
      "         1.0       0.74      0.65      0.70       332\n",
      "\n",
      "    accuracy                           0.99     13067\n",
      "   macro avg       0.87      0.82      0.84     13067\n",
      "weighted avg       0.98      0.99      0.99     13067\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# compute prediction results in the form of numpy ndarray\n",
    "model.eval()\n",
    "true_list, preds_list = [], []\n",
    "for x_test, y_test in DataLoader(dataset=test_ds, batch_size=1, shuffle=False):\n",
    "    with torch.no_grad():\n",
    "        true_list.append(y_test.detach().numpy())\n",
    "        preds = model(x_test)\n",
    "        preds = torch.sigmoid(preds)\n",
    "        preds_list.append(preds.detach().numpy())\n",
    "\n",
    "test_true_np, test_preds_np = np.concatenate(true_list), np.concatenate(preds_list)\n",
    "\n",
    "# compute evaluation metrics including ROC AUC score, accuracy score, f1-score, precision and recall\n",
    "print(\"MODEL PERFORMANCE ON TEST DATASET\")\n",
    "print(f'\\n\\nROC AUC score: {round(roc_auc_score(test_true_np, test_preds_np), 3)}')\n",
    "print(classification_report(y_true=test_true_np, y_pred=(test_preds_np > threshold).astype(int)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, our model has shows consistent performance for when tested on both training and test set. This shows that the model does not suffer from overfitting issue. Also, the accuracy score is quite satisfactory and shows no indication of underfitting. Nevertheless, the recall for class 1 is on the down side. This could be due to the imbalance class distribution that was found out earlier. You can try to improve the model performance by tackling the class imbalance issue, for example using undersampling or oversampling techinque. Next, we will be going into the exercise section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"exercise\">Exercise</a>\n",
    "\n",
    "This exercise section is attached for you to practice and hone your skills. Try your best effort to complete it without referring to the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Configurations\n",
    "You can import any module that you wish to use for this exercise. However, if you run the cells from the start of this notebook, most probably all the modules which you need had already been imported at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "**INSTRUCTIONS**: Please perform binary classification task using the same dataset and features but instead choose the target variable of \"label2\". Feel free to experiment with other features or use feature engineering techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import training and test dataset\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Analytics\n",
    "**INSTRUCTIONS**: Perform some basic analytics on the dataset by following the guidelines that are provided in each cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataset is: None\n",
      "Rows of train data are: None\n",
      "Columns of train data are: None\n"
     ]
    }
   ],
   "source": [
    "# compute the number of rows and columns of train data\n",
    "### BEGIN SOLUTION\n",
    "df_train_shape = None\n",
    "df_train_rows = None\n",
    "df_train_columns = None\n",
    "### END SOLUTION\n",
    "\n",
    "print(f\"Shape of dataset is: {df_train_shape}\")\n",
    "print(\"Rows of train data are: \" + str(df_train_rows))\n",
    "print(\"Columns of train data are: \" + str(df_train_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect first 5 rows of training data\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes in train dataset: None\n"
     ]
    }
   ],
   "source": [
    "# compute class distribution\n",
    "### BEGIN SOLUTION\n",
    "class_distribution = None\n",
    "### END SOLUTION\n",
    "\n",
    "print('Classes in train dataset:', class_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect missing values\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the unnecessary columns\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for the data types\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate out features and target variable\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data pre-processing : min-max normalization\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Development\n",
    "**INSTRUCTIONS**: Prepare the input data and define your own LSTM model. Implement training and validation loop at each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a helper function which slices through the features using the sequence_length parameter and index the labe at the time step\n",
    "# function should return features in sequence and labels\n",
    "def sequencing_data(x_data, y_data, sequence_length):\n",
    "    \"\"\"\n",
    "    Helper function to sample sub-sequence of training data.\n",
    "    Input data must be numpy.\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    pass\n",
    "    ### END SOLUTION\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "# calling the helper function and store them in variables\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "\n",
    "### END SOLUTION\n",
    "\n",
    "# sanity check \n",
    "### BEGIN SOLUTION\n",
    "# print(\"Total samples for X train: \" + str(len(X_sequence_train)))\n",
    "# print(\"Total samples for y train: \" + str(len(y_sequence_train)))\n",
    "# print(\"Total samples for X test: \" + str(len(X_sequence_test)))\n",
    "# print(\"Total samples for y test: \" + str(len(y_sequence_test)))\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a customized Dataset class\n",
    "class MaintenanceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Convert input data into torch FloatTensor. \n",
    "    Inherit Dataset class. Return length of instance when len method is called and return specific sample\n",
    "    of data when indexed.\n",
    "    \"\"\"    \n",
    "    ### BEGIN SOLUTION\n",
    "    pass\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate Dataset object\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate DataLoader object\n",
    "### BEGIN SOLUTION\n",
    "batch_size = None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model architecture is: \n",
      "\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# defining model architecture\n",
    "class LSTM(nn.Module):\n",
    "    ### BEGIN SOLUTION\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ### END SOLUTION\n",
    "\n",
    "# declaring number of layers, number of classes/output categories, input size and hidden size\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### END SOLUTION\n",
    "\n",
    "# instantiate LSTM object\n",
    "### BEGIN SOLUTION\n",
    "model = None\n",
    "### END SOLUTION\n",
    "\n",
    "print(\"Model architecture is: \\n\\n\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyperparameters for model\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement model training and validation loop\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss score for training and validation loop\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "**INSTRUCTIONS:** Follow the guidelines given in the cells to evaluate performance of model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute prediction results in the form of numpy ndarray and evaluation metrics including accuracy score, f1-score, precision and recall\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"reference\">Reference</a>:\n",
    "1. [Deep Learning for tabular data using Pytorch](https://jovian.ai/aakanksha-ns/shelter-outcome)\n",
    "2. [pytorch custom dataset: DataLoader returns a list of tensors rather than tensor of a list](https://stackoverflow.com/questions/62208904/pytorch-custom-dataset-dataloader-returns-a-list-of-tensors-rather-than-tensor)\n",
    "3. [Predictive maintenance - NASA Turbofan Dataset](https://www.kaggle.com/c/predictive-maintenance)\n",
    "4. [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"bonus\">Bonus</a>\n",
    "We can also just load one batch of DataLoader as checking, by converting DataLoader to iterator and calling `next` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[0.0000, 0.0000, 0.4598,  ..., 0.7132, 0.7247, 0.0000],\n",
       "          [0.0000, 0.0028, 0.6092,  ..., 0.6667, 0.7310, 0.0028],\n",
       "          [0.0000, 0.0055, 0.2529,  ..., 0.6279, 0.6214, 0.0055],\n",
       "          ...,\n",
       "          [0.0000, 0.0748, 0.3621,  ..., 0.6744, 0.5384, 0.0748],\n",
       "          [0.0000, 0.0776, 0.5690,  ..., 0.6124, 0.6428, 0.0776],\n",
       "          [0.0000, 0.0803, 0.3736,  ..., 0.7054, 0.7136, 0.0803]],\n",
       " \n",
       "         [[0.0000, 0.0028, 0.6092,  ..., 0.6667, 0.7310, 0.0028],\n",
       "          [0.0000, 0.0055, 0.2529,  ..., 0.6279, 0.6214, 0.0055],\n",
       "          [0.0000, 0.0083, 0.5402,  ..., 0.5736, 0.6624, 0.0083],\n",
       "          ...,\n",
       "          [0.0000, 0.0776, 0.5690,  ..., 0.6124, 0.6428, 0.0776],\n",
       "          [0.0000, 0.0803, 0.3736,  ..., 0.7054, 0.7136, 0.0803],\n",
       "          [0.0000, 0.0831, 0.5805,  ..., 0.6202, 0.6091, 0.0831]],\n",
       " \n",
       "         [[0.0000, 0.0055, 0.2529,  ..., 0.6279, 0.6214, 0.0055],\n",
       "          [0.0000, 0.0083, 0.5402,  ..., 0.5736, 0.6624, 0.0083],\n",
       "          [0.0000, 0.0111, 0.3908,  ..., 0.5891, 0.7045, 0.0111],\n",
       "          ...,\n",
       "          [0.0000, 0.0803, 0.3736,  ..., 0.7054, 0.7136, 0.0803],\n",
       "          [0.0000, 0.0831, 0.5805,  ..., 0.6202, 0.6091, 0.0831],\n",
       "          [0.0000, 0.0859, 0.5287,  ..., 0.6822, 0.8364, 0.0859]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.0101, 0.0139, 0.4425,  ..., 0.7829, 0.7357, 0.0139],\n",
       "          [0.0101, 0.0166, 0.5057,  ..., 0.7442, 0.7976, 0.0166],\n",
       "          [0.0101, 0.0194, 0.5862,  ..., 0.7519, 0.7510, 0.0194],\n",
       "          ...,\n",
       "          [0.0101, 0.0886, 0.3161,  ..., 0.6744, 0.7584, 0.0886],\n",
       "          [0.0101, 0.0914, 0.7414,  ..., 0.5659, 0.7773, 0.0914],\n",
       "          [0.0101, 0.0942, 0.6149,  ..., 0.7132, 0.8756, 0.0942]],\n",
       " \n",
       "         [[0.0101, 0.0166, 0.5057,  ..., 0.7442, 0.7976, 0.0166],\n",
       "          [0.0101, 0.0194, 0.5862,  ..., 0.7519, 0.7510, 0.0194],\n",
       "          [0.0101, 0.0222, 0.5977,  ..., 0.7132, 0.8192, 0.0222],\n",
       "          ...,\n",
       "          [0.0101, 0.0914, 0.7414,  ..., 0.5659, 0.7773, 0.0914],\n",
       "          [0.0101, 0.0942, 0.6149,  ..., 0.7132, 0.8756, 0.0942],\n",
       "          [0.0101, 0.0970, 0.5747,  ..., 0.7597, 0.6820, 0.0970]],\n",
       " \n",
       "         [[0.0101, 0.0194, 0.5862,  ..., 0.7519, 0.7510, 0.0194],\n",
       "          [0.0101, 0.0222, 0.5977,  ..., 0.7132, 0.8192, 0.0222],\n",
       "          [0.0101, 0.0249, 0.2414,  ..., 0.8217, 0.9753, 0.0249],\n",
       "          ...,\n",
       "          [0.0101, 0.0942, 0.6149,  ..., 0.7132, 0.8756, 0.0942],\n",
       "          [0.0101, 0.0970, 0.5747,  ..., 0.7597, 0.6820, 0.0970],\n",
       "          [0.0101, 0.0997, 0.7874,  ..., 0.7287, 0.7049, 0.0997]]]),\n",
       " tensor([[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]])]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
