{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO : \n",
    "- add prediction graph\n",
    "- add loss/epoch graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariate LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages needed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data sequencing function \n",
    "def univariate_single_step(sequence, window_size):\n",
    "    x, y = list(), list()\n",
    "    for i in range(len(sequence)):\n",
    "    # find the end of this pattern\n",
    "        end_ix = i + window_size\n",
    "        # check if we are beyond the sequence\n",
    "        if end_ix > len(sequence)-1:\n",
    "            break\n",
    "    # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
    "        x.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(x), np.array(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x201b9e1d590>"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_ratio = 0.70\n",
    "num_epochs = 60\n",
    "\n",
    "\n",
    "#seed\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([  0,  10,  20,  30,  40,  50,  60,  70,  80,  90, 100, 110, 120,\n",
       "       130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240, 250,\n",
       "       260, 270, 280, 290, 300, 310, 320, 330, 340, 350, 360, 370, 380,\n",
       "       390])"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Synthetic Data\n",
    "univariate_series = np.array([x for x in range(0, 400, 10)])\n",
    "print(univariate_series.shape)\n",
    "univariate_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_data = round(len(univariate_series)*split_ratio)\n",
    "split_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data_shape\n",
      "(28,)\n",
      "test_data_shape\n",
      "(12,)\n"
     ]
    }
   ],
   "source": [
    "#split data by indexing \n",
    "train_data = univariate_series[:split_data]\n",
    "test_data = univariate_series[split_data:]\n",
    "print(\"train_data_shape\")\n",
    "print(train_data.shape)\n",
    "print(\"test_data_shape\")\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "train_data_normalized = scaler.fit_transform(train_data.reshape(-1, 1))\n",
    "\n",
    "test_data_normalized = scaler.fit_transform(test_data.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sequencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainX shape:(25, 3, 1) trainY shape:(25, 1)\n",
      "\n",
      "testX shape:(9, 3, 1) testX shape:(9, 3, 1)\n"
     ]
    }
   ],
   "source": [
    "trainX ,trainY =  univariate_single_step(train_data_normalized,3)\n",
    "testX , testY = univariate_single_step(test_data_normalized,3)\n",
    "print(f\"trainX shape:{trainX.shape} trainY shape:{trainY.shape}\\n\")\n",
    "print(f\"testX shape:{testX.shape} testX shape:{testX.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainX shape:torch.Size([25, 3, 1]) trainY shape:torch.Size([25, 1])\n",
      "\n",
      "testX shape:torch.Size([9, 3, 1]) testX shape:torch.Size([9, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "#transfrom to Pytorch tensor\n",
    "trainX = torch.as_tensor(trainX).float()\n",
    "trainY = torch.as_tensor(trainY).float()\n",
    "testX = torch.as_tensor(testX).float()\n",
    "testY = torch.as_tensor(testY).float()\n",
    "print(f\"trainX shape:{trainX.shape} trainY shape:{trainY.shape}\\n\")\n",
    "print(f\"testX shape:{testX.shape} testX shape:{testX.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features are now in the shape of torch.Size([25, 3, 1]) while labels are now in the shape of torch.Size([25, 1])\n",
      "\n",
      "x-feature\n",
      "25 = total number of data \n",
      "3 = window size \n",
      "1 = number of time series\n",
      "\n",
      "y-label\n",
      "25 = number of data\n",
      "1 = number of step\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Features are now in the shape of {trainX.shape} while labels are now in the shape of {trainY.shape}\\n\")\n",
    "print(\"x-feature\\n\"+str(trainX.shape[0])+\" = total number of data \")\n",
    "print(str(trainX.shape[1])+\" = window size \")\n",
    "print(str(trainX.shape[2])+\" = number of time series\\n\")\n",
    "print(\"y-label\\n\"+str(trainY.shape[0])+\" = number of data\")\n",
    "print(str(trainY.shape[1])+\" = number of step\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model Configuration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanila LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "\n",
    "        def __init__(self, n_feature, hidden_dim, num_layers, output_dim):\n",
    "            super(LSTM, self).__init__()\n",
    "\n",
    "            self.n_feature = n_feature\n",
    "            # Hidden dimensions\n",
    "            self.hidden_dim = hidden_dim\n",
    "\n",
    "            # Number of hidden layers\n",
    "            self.num_layers = num_layers\n",
    "\n",
    "            # Building your LSTM\n",
    "            # batch_first=True causes input/output tensors to be of shape\n",
    "            # (batch_dim, seq_dim, feature_dim)\n",
    "            self.lstm = nn.LSTM(n_feature, hidden_dim, num_layers, batch_first=True)\n",
    "\n",
    "            # Readout layer\n",
    "            self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "\n",
    "        def forward(self, x):\n",
    "            # Initialize hidden state with zeros\n",
    "            h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "            # Initialize cell state\n",
    "            c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "            # One time step\n",
    "            # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
    "            # If we don't, we'll backprop all the way to the start even after going through another batch\n",
    "            out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "\n",
    "            # Index hidden state of last time step\n",
    "            # out.size() --> 100, 28, 100\n",
    "            # out[:, -1, :] --> 100, 100 --> just want last time step hidden states!\n",
    "            out = self.fc(out[:, -1, :])\n",
    "            # out.size() --> 100, 10\n",
    "            return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BidirectionalLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, n_feature, hidden_dim, num_layers, output_dim):\n",
    "        super(BidirectionalLSTM, self).__init__()\n",
    "\n",
    "        self.n_feature = n_feature\n",
    "        # Hidden dimensions\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Number of hidden layers\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Building your LSTM\n",
    "        # batch_first=True causes input/output tensors to be of shape\n",
    "        # (batch_dim, seq_dim, feature_dim)\n",
    "        self.lstm = nn.LSTM(n_feature, hidden_dim, num_layers, batch_first=True,bidirectional=True)\n",
    "\n",
    "        # Readout layer *2 for bidirectional LSTM\n",
    "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        # Initialize cell state\n",
    "        c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
    "        # If we don't, we'll backprop all the way to the start even after going through another batch\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "\n",
    "        # Index hidden state of last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Arguments for LSTM model\n",
    "hidden_dim = 10\n",
    "number_of_time_series = 1 \n",
    "timestep = 1\n",
    "output_dim =1 \n",
    "\n",
    "#1 for vanila LSTM , >1 is mean stacked LSTM\n",
    "num_layers = 3 \n",
    "\n",
    "#Vanila , Stacked LSTM\n",
    "# model = LSTM(n_feature=number_of_time_series, hidden_dim=hidden_dim, output_dim=timestep, num_layers=num_layers)\n",
    "\n",
    "#Bidirectional LSTM\n",
    "model = BidirectionalLSTM(n_feature=number_of_time_series, hidden_dim=hidden_dim, output_dim=timestep, num_layers=num_layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss function \n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "#optimiser\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 MSE:  0.345790296792984\n",
      "Epoch  1 MSE:  0.3182530701160431\n",
      "Epoch  2 MSE:  0.29817891120910645\n",
      "Epoch  3 MSE:  0.28501227498054504\n",
      "Epoch  4 MSE:  0.2792040705680847\n",
      "Epoch  5 MSE:  0.27944812178611755\n",
      "Epoch  6 MSE:  0.2782609462738037\n",
      "Epoch  7 MSE:  0.27061522006988525\n",
      "Epoch  8 MSE:  0.2579366862773895\n",
      "Epoch  9 MSE:  0.2427166998386383\n",
      "Epoch  10 MSE:  0.22584863007068634\n",
      "Epoch  11 MSE:  0.20653007924556732\n",
      "Epoch  12 MSE:  0.18197958171367645\n",
      "Epoch  13 MSE:  0.14810732007026672\n",
      "Epoch  14 MSE:  0.10541140288114548\n",
      "Epoch  15 MSE:  0.06541479378938675\n",
      "Epoch  16 MSE:  0.045050300657749176\n",
      "Epoch  17 MSE:  0.0393492691218853\n",
      "Epoch  18 MSE:  0.03449425846338272\n",
      "Epoch  19 MSE:  0.036943886429071426\n",
      "Epoch  20 MSE:  0.05139799043536186\n",
      "Epoch  21 MSE:  0.059045687317848206\n",
      "Epoch  22 MSE:  0.0520176887512207\n",
      "Epoch  23 MSE:  0.03793324530124664\n",
      "Epoch  24 MSE:  0.025310097262263298\n",
      "Epoch  25 MSE:  0.018119577318429947\n",
      "Epoch  26 MSE:  0.016170918941497803\n",
      "Epoch  27 MSE:  0.017468174919486046\n",
      "Epoch  28 MSE:  0.02011590451002121\n",
      "Epoch  29 MSE:  0.022846974432468414\n",
      "Epoch  30 MSE:  0.024773063138127327\n",
      "Epoch  31 MSE:  0.02523825690150261\n",
      "Epoch  32 MSE:  0.023943772539496422\n",
      "Epoch  33 MSE:  0.021088290959596634\n",
      "Epoch  34 MSE:  0.01736840046942234\n",
      "Epoch  35 MSE:  0.013822803273797035\n",
      "Epoch  36 MSE:  0.011510205455124378\n",
      "Epoch  37 MSE:  0.011030538938939571\n",
      "Epoch  38 MSE:  0.012068193405866623\n",
      "Epoch  39 MSE:  0.013386190868914127\n",
      "Epoch  40 MSE:  0.013579056598246098\n",
      "Epoch  41 MSE:  0.012108037248253822\n",
      "Epoch  42 MSE:  0.009631341323256493\n",
      "Epoch  43 MSE:  0.007349725812673569\n",
      "Epoch  44 MSE:  0.006070590112358332\n",
      "Epoch  45 MSE:  0.005798729602247477\n",
      "Epoch  46 MSE:  0.006030116695910692\n",
      "Epoch  47 MSE:  0.006282688584178686\n",
      "Epoch  48 MSE:  0.006333481520414352\n",
      "Epoch  49 MSE:  0.006092740222811699\n",
      "Epoch  50 MSE:  0.005461552180349827\n",
      "Epoch  51 MSE:  0.004458749666810036\n",
      "Epoch  52 MSE:  0.0034231918398290873\n",
      "Epoch  53 MSE:  0.002844985108822584\n",
      "Epoch  54 MSE:  0.0028711799532175064\n",
      "Epoch  55 MSE:  0.003119373694062233\n",
      "Epoch  56 MSE:  0.003107199678197503\n",
      "Epoch  57 MSE:  0.002725589321926236\n",
      "Epoch  58 MSE:  0.002165476093068719\n",
      "Epoch  59 MSE:  0.0016092787263914943\n"
     ]
    }
   ],
   "source": [
    "for t in range(num_epochs):\n",
    "    # Initialise hidden state\n",
    "#     Don't do this if you want your LSTM to be stateful\n",
    "#     model.hidden = model.init_hidden()\n",
    "\n",
    "    # Forward pass\n",
    "    y_train_pred = model(trainX)\n",
    "    # print(\"after transform y_train_pred.shape\"+str(y_train_pred.shape))\n",
    "\n",
    "    loss = loss_fn(y_train_pred, trainY)\n",
    "    print(\"Epoch \", t, \"MSE: \", loss.item())\n",
    "\n",
    "    # Zero out gradient, else they will accumulate between epochs\n",
    "    optimiser.zero_grad()\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Update parameters\n",
    "    optimiser.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "y_test_pred = model(testX)\n",
    "\n",
    "#Reshape to original data\n",
    "y_train_pred = torch.reshape(y_train_pred,(y_train_pred.shape[0],y_train_pred.shape[1]))\n",
    "trainY = torch.reshape(trainY,(trainY.shape[0],trainY.shape[1]))\n",
    "y_test_pred = torch.reshape(y_test_pred,(y_test_pred.shape[0],y_test_pred.shape[1]))\n",
    "testY = torch.reshape(testY,(testY.shape[0],testY.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Invert predictions\n",
    "y_train_pred = scaler.inverse_transform(y_train_pred.detach().numpy())\n",
    "y_train = scaler.inverse_transform(trainY.detach().numpy())\n",
    "y_test_pred = scaler.inverse_transform(y_test_pred.detach().numpy())\n",
    "y_test = scaler.inverse_transform(testY.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y-test\t\ty-predict\n",
      "[522.1]\t\t[507.70514]\n",
      "[537.7]\t\t[518.5679]\n",
      "[553.3]\t\t[532.7378]\n",
      "[568.9]\t\t[549.4688]\n",
      "[584.5]\t\t[567.1227]\n",
      "[600.10004]\t\t[584.54034]\n",
      "[615.7]\t\t[601.3206]\n",
      "[631.3]\t\t[616.9207]\n",
      "[646.9]\t\t[630.35736]\n"
     ]
    }
   ],
   "source": [
    "print(\"y-test\\t\\ty-predict\")\n",
    "for i in range(len(y_test_pred)):\n",
    "    print(f\"{y_test[i]}\\t\\t{y_test_pred[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test_shape : (9, 1)\n",
      "y_test_pred_shape : (9, 1)\n"
     ]
    }
   ],
   "source": [
    "print(f\"y_test_shape : {y_test.shape}\")\n",
    "print(f\"y_test_pred_shape : {y_test_pred.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 3.44 RMSE\n",
      "Test Score: 17.01 RMSE\n"
     ]
    }
   ],
   "source": [
    "# calculate root mean squared error\n",
    "trainScore = math.sqrt(mean_squared_error(y_train[:,0], y_train_pred[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(y_test[:,0], y_test_pred[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise for Univariate (Solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using LSTM to create a model that can predict lastest sales of shampoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x201b9e1d590>"
      ]
     },
     "execution_count": 528,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Hyperparameter\n",
    "num_epochs_shampoo = 100\n",
    "split_ratio = 0.70\n",
    "\n",
    "#Hidden Layer for LSTM\n",
    "hidden_dim = 32\n",
    "\n",
    "#seed\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    266.0\n",
       "1    145.9\n",
       "2    183.1\n",
       "3    119.3\n",
       "4    180.3\n",
       "Name: sales, dtype: float64"
      ]
     },
     "execution_count": 513,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shampoo = pd.read_csv('../datasets/others/shampoo-sales.csv')\n",
    "shampoo_ts =shampoo['sales']\n",
    "shampoo_ts.head() \n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 514,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#split data by indexing \n",
    "split_data = round(len(shampoo_ts)*split_ratio)\n",
    "split_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data_shampoo_shape\n",
      "(25,)\n",
      "test_data_shampoo_shape\n",
      "(11,)\n"
     ]
    }
   ],
   "source": [
    "train_data_shampoo = shampoo_ts[:split_data]\n",
    "test_data_shampoo = shampoo_ts[split_data:]\n",
    "print(\"train_data_shampoo_shape\")\n",
    "print(train_data_shampoo.shape)\n",
    "print(\"test_data_shampoo_shape\")\n",
    "print(test_data_shampoo.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.82401588],\n",
       "       [-0.57790275],\n",
       "       [-1.        ],\n",
       "       [-0.59642739],\n",
       "       [-0.67449553]])"
      ]
     },
     "execution_count": 516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data Normalization\n",
    "\n",
    "#Reshape before normalize\n",
    "train_data_shampoo = train_data_shampoo.values.reshape(-1, 1)\n",
    "test_data_shampoo = test_data_shampoo.values.reshape((-1, 1))\n",
    "\n",
    "#Build Scaler\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "train_data_normalized_shampoo = scaler.fit_transform(train_data_shampoo)\n",
    "\n",
    "test_data_normalized_shampoo = scaler.fit_transform(test_data_shampoo)\n",
    "train_data_normalized_shampoo[1:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainX shape:(23, 2, 1) trainY shape:(23, 1)\n",
      "\n",
      "testX shape:(9, 2, 1) testX shape:(9, 1)\n"
     ]
    }
   ],
   "source": [
    "#Data Sequencing \n",
    "trainX_shampoo ,trainY_shampoo =  univariate_single_step(train_data_normalized_shampoo,2)\n",
    "testX_shampoo , testY_shampoo = univariate_single_step(test_data_normalized_shampoo,2)\n",
    "print(f\"trainX shape:{trainX_shampoo.shape} trainY shape:{trainY_shampoo.shape}\\n\")\n",
    "print(f\"testX shape:{testX_shampoo.shape} testX shape:{testY_shampoo.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainX shape:torch.Size([23, 2, 1]) trainY shape:torch.Size([23, 1])\n",
      "\n",
      "testX shape:torch.Size([9, 2, 1]) testX shape:torch.Size([9, 1])\n"
     ]
    }
   ],
   "source": [
    "#Transfrom numpy to Pytorch tensor\n",
    "trainX_shampoo = torch.as_tensor(trainX_shampoo).float()\n",
    "trainY_shampoo = torch.as_tensor(trainY_shampoo).float()\n",
    "testX_shampoo = torch.as_tensor(testX_shampoo).float()\n",
    "testY_shampoo = torch.as_tensor(testY_shampoo).float()\n",
    "print(f\"trainX shape:{trainX_shampoo.shape} trainY shape:{trainY_shampoo.shape}\\n\")\n",
    "print(f\"testX shape:{testX_shampoo.shape} testX shape:{testY_shampoo.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BidirectionalLSTM(\n",
       "  (lstm): LSTM(1, 32, batch_first=True, bidirectional=True)\n",
       "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 536,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Arguments for LSTM model\n",
    "number_of_time_series = 1 \n",
    "timestep = 1\n",
    "output_dim =1 \n",
    "#1 for vanila LSTM , >1 is mean stacked LSTM\n",
    "num_layers = 1\n",
    "\n",
    "#Vanila ,Stacked LSTM\n",
    "# model_shampoo = LSTM(n_feature=number_of_time_series, hidden_dim=hidden_dim, output_dim=timestep, num_layers=num_layers)\n",
    "\n",
    "#Bidirectional LSTM\n",
    "model_shampoo = BidirectionalLSTM(n_feature=number_of_time_series, hidden_dim=hidden_dim, output_dim=timestep, num_layers=num_layers)\n",
    "model_shampoo.float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn_shampoo = torch.nn.MSELoss()\n",
    "\n",
    "optimiser_shampoo = torch.optim.Adam(model_shampoo.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 MSE:  0.2840895652770996\n",
      "Epoch  1 MSE:  0.26358506083488464\n",
      "Epoch  2 MSE:  0.2513701319694519\n",
      "Epoch  3 MSE:  0.2455216944217682\n",
      "Epoch  4 MSE:  0.24335384368896484\n",
      "Epoch  5 MSE:  0.240908682346344\n",
      "Epoch  6 MSE:  0.23643222451210022\n",
      "Epoch  7 MSE:  0.2305239588022232\n",
      "Epoch  8 MSE:  0.22434689104557037\n",
      "Epoch  9 MSE:  0.2187640517950058\n",
      "Epoch  10 MSE:  0.21409872174263\n",
      "Epoch  11 MSE:  0.21003389358520508\n",
      "Epoch  12 MSE:  0.20569400489330292\n",
      "Epoch  13 MSE:  0.20033761858940125\n",
      "Epoch  14 MSE:  0.19427035748958588\n",
      "Epoch  15 MSE:  0.18886412680149078\n",
      "Epoch  16 MSE:  0.1854628473520279\n",
      "Epoch  17 MSE:  0.1835416704416275\n",
      "Epoch  18 MSE:  0.18142260611057281\n",
      "Epoch  19 MSE:  0.17933984100818634\n",
      "Epoch  20 MSE:  0.17924568057060242\n",
      "Epoch  21 MSE:  0.18108268082141876\n",
      "Epoch  22 MSE:  0.1816246062517166\n",
      "Epoch  23 MSE:  0.18050965666770935\n",
      "Epoch  24 MSE:  0.1792011260986328\n",
      "Epoch  25 MSE:  0.17752191424369812\n",
      "Epoch  26 MSE:  0.17484575510025024\n",
      "Epoch  27 MSE:  0.1721193939447403\n",
      "Epoch  28 MSE:  0.1704719513654709\n",
      "Epoch  29 MSE:  0.16982264816761017\n",
      "Epoch  30 MSE:  0.1695115864276886\n",
      "Epoch  31 MSE:  0.16926467418670654\n",
      "Epoch  32 MSE:  0.16919104754924774\n",
      "Epoch  33 MSE:  0.16932782530784607\n",
      "Epoch  34 MSE:  0.16944250464439392\n",
      "Epoch  35 MSE:  0.16924938559532166\n",
      "Epoch  36 MSE:  0.16870681941509247\n",
      "Epoch  37 MSE:  0.16802921891212463\n",
      "Epoch  38 MSE:  0.16745324432849884\n",
      "Epoch  39 MSE:  0.16704192757606506\n",
      "Epoch  40 MSE:  0.166725754737854\n",
      "Epoch  41 MSE:  0.1665007472038269\n",
      "Epoch  42 MSE:  0.1664750576019287\n",
      "Epoch  43 MSE:  0.16665877401828766\n",
      "Epoch  44 MSE:  0.16683733463287354\n",
      "Epoch  45 MSE:  0.16680017113685608\n",
      "Epoch  46 MSE:  0.1665789932012558\n",
      "Epoch  47 MSE:  0.16631972789764404\n",
      "Epoch  48 MSE:  0.16607129573822021\n",
      "Epoch  49 MSE:  0.1658223420381546\n",
      "Epoch  50 MSE:  0.16561560332775116\n",
      "Epoch  51 MSE:  0.16551101207733154\n",
      "Epoch  52 MSE:  0.16549013555049896\n",
      "Epoch  53 MSE:  0.16546602547168732\n",
      "Epoch  54 MSE:  0.1653745323419571\n",
      "Epoch  55 MSE:  0.16521815955638885\n",
      "Epoch  56 MSE:  0.16502782702445984\n",
      "Epoch  57 MSE:  0.16481809318065643\n",
      "Epoch  58 MSE:  0.16459248960018158\n",
      "Epoch  59 MSE:  0.164371058344841\n",
      "Epoch  60 MSE:  0.16418516635894775\n",
      "Epoch  61 MSE:  0.16404134035110474\n",
      "Epoch  62 MSE:  0.16390962898731232\n",
      "Epoch  63 MSE:  0.16375671327114105\n",
      "Epoch  64 MSE:  0.16357584297657013\n",
      "Epoch  65 MSE:  0.16337594389915466\n",
      "Epoch  66 MSE:  0.16316065192222595\n",
      "Epoch  67 MSE:  0.16293200850486755\n",
      "Epoch  68 MSE:  0.16270169615745544\n",
      "Epoch  69 MSE:  0.16248326003551483\n",
      "Epoch  70 MSE:  0.16227640211582184\n",
      "Epoch  71 MSE:  0.16206732392311096\n",
      "Epoch  72 MSE:  0.16184356808662415\n",
      "Epoch  73 MSE:  0.1616016924381256\n",
      "Epoch  74 MSE:  0.16134272515773773\n",
      "Epoch  75 MSE:  0.1610667109489441\n",
      "Epoch  76 MSE:  0.16077455878257751\n",
      "Epoch  77 MSE:  0.16047070920467377\n",
      "Epoch  78 MSE:  0.16015978157520294\n",
      "Epoch  79 MSE:  0.15984053909778595\n",
      "Epoch  80 MSE:  0.15950627624988556\n",
      "Epoch  81 MSE:  0.1591508686542511\n",
      "Epoch  82 MSE:  0.15877237915992737\n",
      "Epoch  83 MSE:  0.15837106108665466\n",
      "Epoch  84 MSE:  0.15794694423675537\n",
      "Epoch  85 MSE:  0.15750068426132202\n",
      "Epoch  86 MSE:  0.15703396499156952\n",
      "Epoch  87 MSE:  0.15654706954956055\n",
      "Epoch  88 MSE:  0.1560365855693817\n",
      "Epoch  89 MSE:  0.15549741685390472\n",
      "Epoch  90 MSE:  0.154926136136055\n",
      "Epoch  91 MSE:  0.15432199835777283\n",
      "Epoch  92 MSE:  0.15368539094924927\n",
      "Epoch  93 MSE:  0.15301670134067535\n",
      "Epoch  94 MSE:  0.15231645107269287\n",
      "Epoch  95 MSE:  0.1515842080116272\n",
      "Epoch  96 MSE:  0.1508176028728485\n",
      "Epoch  97 MSE:  0.1500130146741867\n",
      "Epoch  98 MSE:  0.1491682231426239\n",
      "Epoch  99 MSE:  0.14828209578990936\n"
     ]
    }
   ],
   "source": [
    "for t in range(num_epochs_shampoo):\n",
    "    # Initialise hidden state\n",
    "    # Don't do this if you want your LSTM to be stateful\n",
    "    # model.hidden = model.init_hidden()\n",
    "\n",
    "    # Forward pass\n",
    "    y_train_pred_shampoo = model_shampoo(trainX_shampoo)\n",
    "    loss_shampoo = loss_fn_shampoo(y_train_pred_shampoo, trainY_shampoo)\n",
    "    print(\"Epoch \", t, \"MSE: \", loss_shampoo.item())\n",
    "\n",
    "    # Zero out gradient, else they will accumulate between epochs\n",
    "    optimiser_shampoo.zero_grad()\n",
    "\n",
    "    # Backward pass\n",
    "    loss_shampoo.backward()\n",
    "\n",
    "    # Update parameters\n",
    "    optimiser_shampoo.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make predictions\n",
    "y_test_pred_shampoo = model_shampoo(testX_shampoo)\n",
    "\n",
    "#Reshape to original data\n",
    "y_train_pred_shampoo = torch.reshape(y_train_pred_shampoo,(y_train_pred_shampoo.shape[0],y_train_pred_shampoo.shape[1]))\n",
    "trainY_shampoo = torch.reshape(trainY_shampoo,(trainY_shampoo.shape[0],trainY_shampoo.shape[1]))\n",
    "y_test_pred_shampoo = torch.reshape(y_test_pred_shampoo,(y_test_pred_shampoo.shape[0],y_test_pred_shampoo.shape[1]))\n",
    "testY_shampoo = torch.reshape(testY_shampoo,(testY_shampoo.shape[0],testY_shampoo.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Invert predictions\n",
    "y_train_pred_shampoo = scaler.inverse_transform(y_train_pred_shampoo.detach().numpy())\n",
    "y_train_shampoo = scaler.inverse_transform(trainY_shampoo.detach().numpy())\n",
    "y_test_pred_shampoo = scaler.inverse_transform(y_test_pred_shampoo.detach().numpy())\n",
    "y_test_shampoo = scaler.inverse_transform(testY_shampoo.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 1)"
      ]
     },
     "execution_count": 541,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_shampoo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y-test\t\ty-predict\n",
      "[439.3]\t\t[424.71152]\n",
      "[401.30002]\t\t[393.35703]\n",
      "[437.4]\t\t[437.54062]\n",
      "[575.5]\t\t[425.14606]\n",
      "[407.60004]\t\t[481.2224]\n",
      "[682.]\t\t[496.14194]\n",
      "[475.3]\t\t[464.71143]\n",
      "[581.3]\t\t[566.05054]\n",
      "[646.9]\t\t[524.4608]\n"
     ]
    }
   ],
   "source": [
    "print(\"y-test\\t\\ty-predict\")\n",
    "for i in range(len(y_test_shampoo)):\n",
    "    print(f\"{y_test_shampoo[i]}\\t\\t{y_test_pred_shampoo[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test_shape : (9, 1)\n",
      "y_test_pred_shape : (9, 1)\n"
     ]
    }
   ],
   "source": [
    "print(f\"y_test_shape : {y_test_shampoo.shape}\")   \n",
    "print(f\"y_test_pred_shape : {y_test_pred_shampoo.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 70.49 RMSE\n",
      "Test Score: 93.20 RMSE\n"
     ]
    }
   ],
   "source": [
    "#calculate root mean squared error\n",
    "trainScore_shampoo = math.sqrt(mean_squared_error(y_train_shampoo[:,0], y_train_pred_shampoo[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore_shampoo))\n",
    "testScore_shampoo = math.sqrt(mean_squared_error(y_test_shampoo[:,0], y_test_pred_shampoo[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore_shampoo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
