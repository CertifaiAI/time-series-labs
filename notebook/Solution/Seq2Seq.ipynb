{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Do :\n",
    "- Add in live data example \n",
    "- Plot prediction graph \n",
    "- Plot loss and validation curve\n",
    "\n",
    "Bug to slove :\n",
    "- Error in inverse scale transform the predict output "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq Multivariate Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Encoder** is use to accept the multivariate input from the time series \n",
    "- **Decoder** will receive the hidden weight from the encoder state and the other time series to make a prediction \n",
    "- For Example : Given that 3 time series (3 column x-feature) , the encoder will receive all the 3 time series input ,where the decoder will receive the hidden weight from the encoder and the other 2 time series data(n-features) to make 1 time series prediction.The next LSTM unit encoder will take the output from previous unit to feed in as input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"../seq2seq.jpg\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  import libary\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "test_data_size =40\n",
    "seq_length = 2\n",
    "labels_length = 3\n",
    "learning_rate = 1e-2\n",
    "weight_decay = 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_seq1 = np.array([x for x in range(0, 600, 10)])\n",
    "in_seq2 = np.array([x for x in range(5, 605, 10)])\n",
    "out_seq = np.array([in_seq1[i] + in_seq2[i] for i in range(len(in_seq1))])\n",
    "in_seq1 = in_seq1.reshape((len(in_seq1), 1))\n",
    "in_seq2 = in_seq2.reshape((len(in_seq2), 1))\n",
    "out_seq = out_seq.reshape((len(in_seq2), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.hstack((in_seq1, in_seq2,out_seq))\n",
    "dataset= pd.DataFrame(dataset)\n",
    "dataset.columns =[\"Time Series 1\" ,\"Time Series 2\",\"Time Series 3\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data.shape(39, 3)\n",
      "test_data.shape(20, 3)\n"
     ]
    }
   ],
   "source": [
    "train_data = dataset.iloc[1:test_data_size]\n",
    "test_data = dataset.iloc[test_data_size::]\n",
    "print(\"train_data.shape\"+str(train_data.shape))\n",
    "print(\"test_data.shape\"+str(test_data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "train_data_normalized = scaler.fit_transform(train_data)\n",
    "test_data_normalized = scaler.fit_transform(test_data)\n",
    "train_data_normalized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_normalized.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sequencing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_windows(data, seq_length,labels_length):\n",
    "    x = []\n",
    "    y = []\n",
    "    z = []\n",
    "\n",
    "    for i in range(len(data)-(seq_length+labels_length)):\n",
    "        _x = data[i:(i+seq_length),:]\n",
    "        _y = data[(i+seq_length):(i+seq_length+labels_length),0:1]\n",
    "        _z  = data[(i+seq_length):(i+seq_length+labels_length),1:]\n",
    "        x.append(np.array(_x))\n",
    "        y.append(np.array(_y))\n",
    "        z.append(np.array(_z))\n",
    "\n",
    "    return x,y,z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train X  has: 34 data\n",
      "train labels  has: 34 data\n",
      "validiation  X  has: 15 data\n",
      "Validiation  labels  has: 15 data\n"
     ]
    }
   ],
   "source": [
    "train_X, train_y,train_features = sliding_windows(train_data_normalized, seq_length,labels_length)\n",
    "valid_X, valid_y,valid_features = sliding_windows(test_data_normalized,seq_length,labels_length)\n",
    "print(\"train X  has:\", len(train_X) , \"data\")\n",
    "print(\"train labels  has:\", len(train_y) , \"data\")\n",
    "print(\"validiation  X  has:\", len(valid_X) , \"data\")\n",
    "print(\"Validiation  labels  has:\" ,len(valid_y) , \"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainX shape is: torch.Size([34, 2, 3])\n",
      "trainy shape is: torch.Size([34, 3, 1])\n",
      "train features  shape is: torch.Size([34, 3, 2])\n",
      "validX shape is: torch.Size([15, 2, 3])\n",
      "validy shape is: torch.Size([15, 3, 1])\n",
      "valid features  shape is: torch.Size([15, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "trainX =torch.Tensor(train_X)\n",
    "trainy = torch.Tensor(train_y)\n",
    "train_features = torch.Tensor(train_features)\n",
    "validX = torch.Tensor(valid_X)\n",
    "validy= torch.Tensor(valid_y)\n",
    "valid_features = torch.Tensor(valid_features)\n",
    "\n",
    "\n",
    "print (\"trainX shape is:\",trainX.size())\n",
    "print (\"trainy shape is:\",trainy.size())\n",
    "print (\"train features  shape is:\",train_features.size())\n",
    "print (\"validX shape is:\",validX.size())\n",
    "print (\"validy shape is:\",validy.size())\n",
    "print (\"valid features  shape is:\",valid_features.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "#     Configure the encoder\n",
    "    def __init__(self, seq_len, n_features, hidden_dim=64):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.seq_len, self.n_features = seq_len, n_features\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = 3\n",
    "        self.rnn1 = nn.LSTM(\n",
    "            input_size=n_features,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_layers=3,\n",
    "            batch_first=True,\n",
    "\n",
    "        )\n",
    "# Receive trainX input and return hidden & cell information \n",
    "    def forward(self, x):\n",
    "        x = x.reshape((1, self.seq_len, self.n_features))\n",
    "\n",
    "        h_1 = torch.zeros(\n",
    "            self.num_layers, x.size(0), self.hidden_dim)\n",
    "\n",
    "        c_1 = torch.zeros(\n",
    "            self.num_layers, x.size(0), self.hidden_dim)\n",
    "\n",
    "        x, (hidden, cell) = self.rnn1(x, (h_1, c_1))\n",
    "\n",
    "        # return hidden_n.reshape((self.n_features, self.hidden_dim))\n",
    "        return hidden, cell\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "#     Configure the decoder\n",
    "    def __init__(self, seq_len, input_dim=64, n_features=1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.seq_len, self.input_dim = seq_len, input_dim\n",
    "        self.hidden_dim, self.n_features = input_dim, n_features\n",
    "\n",
    "        self.rnn1 = nn.LSTM(\n",
    "            input_size=n_features,\n",
    "            hidden_size=input_dim,\n",
    "            num_layers=3,\n",
    "            batch_first=True,\n",
    "\n",
    "        )\n",
    "\n",
    "        self.output_layer = nn.Linear(self.hidden_dim, n_features)\n",
    "#  Receive Encoder Hidden and Cell \n",
    "    def forward(self, x, input_hidden, input_cell):\n",
    "#         Reshape to ensure output 1 time series\n",
    "        x = x.reshape((1, 1, self.n_features))\n",
    "        # print(\"decode input\",x.size())\n",
    "\n",
    "        x, (hidden_n, cell_n) = self.rnn1(x, (input_hidden, input_cell))\n",
    "\n",
    "        x = self.output_layer(x)\n",
    "        return x, hidden_n, cell_n\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "\n",
    "    def __init__(self, seq_len, n_features,output_length,hidden_dim):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(seq_len, n_features, hidden_dim)\n",
    "        self.n_features = n_features\n",
    "        self.output_length = output_length\n",
    "        self.decoder = Decoder(seq_len, hidden_dim, n_features)\n",
    "\n",
    "    def forward(self, x, prev_y, features):\n",
    "\n",
    "        hidden, cell = self.encoder(x)\n",
    "\n",
    "        # Prepare place holder for decoder output\n",
    "        targets_ta = []\n",
    "        # prev_output become the next input to the LSTM cell\n",
    "        dec_input = prev_y\n",
    "\n",
    "        # dec_input = torch.cat([prev_output, curr_features], dim=1)\n",
    "\n",
    "        # itearate over LSTM - according to the required output days\n",
    "        for out_days in range(self.output_length):\n",
    "\n",
    "            prev_x, prev_hidden, prev_cell = self.decoder(dec_input, hidden, cell)\n",
    "            hidden, cell = prev_hidden, prev_cell\n",
    "\n",
    "            prev_x = prev_x[:, :, 0:1]\n",
    "#             print(\"preve x shape is:\",prev_x.size())\n",
    "            # print(\"features shape is:\",features[out_days+1].size())\n",
    "\n",
    "            if out_days + 1 < self.output_length:\n",
    "                dec_input = torch.cat([prev_x, features[out_days + 1].reshape(1, 1, 2)], dim=2)\n",
    "\n",
    "            targets_ta.append(prev_x.reshape(1))\n",
    "\n",
    "        targets = torch.stack(targets_ta)\n",
    "\n",
    "        return targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (rnn1): LSTM(3, 32, num_layers=3, batch_first=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (rnn1): LSTM(3, 32, num_layers=3, batch_first=True)\n",
      "    (output_layer): Linear(in_features=32, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "n_features = trainX.shape[2]\n",
    "model = Seq2Seq(seq_length, n_features, output_length = labels_length, hidden_dim=32)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (rnn1): LSTM(3, 32, num_layers=3, batch_first=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (rnn1): LSTM(3, 32, num_layers=3, batch_first=True)\n",
       "    (output_layer): Linear(in_features=32, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "\n",
    "\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,weight_decay=weight_decay)\n",
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, TrainX, Trainy, ValidX, Validy, seq_length, n_epochs):\n",
    "    history = dict(train=[], val=[])\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "    \n",
    "        train_losses = []\n",
    "        validate_value= list()\n",
    "        for i in range(len(TrainX)):\n",
    "            seq_inp = TrainX[i, :, :]\n",
    "            seq_true = Trainy[i, :, :]\n",
    "            features = train_features[i, :, :]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            seq_pred_train = model(seq_inp, seq_inp[seq_length - 1:seq_length, :], features)\n",
    "\n",
    "            loss = criterion(seq_pred_train, seq_true)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        val_losses = []\n",
    "        model = model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i in range(len(validX)):\n",
    "                seq_inp = ValidX[i, :, :]\n",
    "                seq_true = Validy[i, :, :]\n",
    "                features = valid_features[i, :, :]\n",
    "\n",
    "                seq_pred_validate = model(seq_inp, seq_inp[seq_length - 1:seq_length, :], features)\n",
    "\n",
    "                loss = criterion(seq_pred_validate, seq_true)\n",
    "                val_losses.append(loss.item())\n",
    "                validate_value.append(seq_pred_validate)\n",
    "\n",
    "        train_loss = np.mean(train_losses)\n",
    "        val_loss = np.mean(val_losses)\n",
    "\n",
    "        history['train'].append(train_loss)\n",
    "        history['val'].append(val_loss)\n",
    "        \n",
    "    \n",
    "    return history,validate_value\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "history,y_test_pred = train_model(\n",
    "    model,\n",
    "    trainX, trainy,\n",
    "    validX, validy,\n",
    "    seq_length,\n",
    "    n_epochs=1,  # Train for few epochs as illustration,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.07222532447571318]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8053216607309878]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history[\"val\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source : https://www.kaggle.com/omershect/learning-pytorch-seq2seq-with-m5-data-set/comments"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
