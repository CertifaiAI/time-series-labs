{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multivariate_univariate_single_step(sequence,window_size):\n",
    "    x, y = list(), list()\n",
    "    for i in range(len(sequence)):\n",
    "    # find the end of this pattern\n",
    "        end_ix = i + window_size\n",
    "        # check if we are beyond the sequence\n",
    "        if end_ix > len(sequence):\n",
    "            break\n",
    "    # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequence[i:end_ix,:-1], sequence[end_ix-1,-1]\n",
    "        x.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(x), np.array(y)\n",
    "\n",
    "\n",
    "\n",
    "def multivariate_univariate_multi_step(sequence,window_size,n_multistep):\n",
    "    x, y = list(), list()\n",
    "    for i in range(len(sequence)):\n",
    "    # find the end of this pattern\n",
    "        end_ix = i + window_size\n",
    "        out_ix = end_ix + n_multistep -1\n",
    "        # check if we are beyond the sequence\n",
    "        if out_ix > len(sequence):\n",
    "            break\n",
    "    # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequence[i:end_ix,:-1], sequence[end_ix-1:out_ix,-1]\n",
    "        x.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(x), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x201371ec590>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_ratio = 0.70\n",
    "num_epochs = 20\n",
    "window_size = 4\n",
    "n_step = 1\n",
    "\n",
    "#seed\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   5,   5],\n",
       "       [ 10,  15,  25],\n",
       "       [ 20,  25,  45],\n",
       "       [ 30,  35,  65],\n",
       "       [ 40,  45,  85],\n",
       "       [ 50,  55, 105],\n",
       "       [ 60,  65, 125],\n",
       "       [ 70,  75, 145],\n",
       "       [ 80,  85, 165],\n",
       "       [ 90,  95, 185],\n",
       "       [100, 105, 205],\n",
       "       [110, 115, 225],\n",
       "       [120, 125, 245],\n",
       "       [130, 135, 265],\n",
       "       [140, 145, 285],\n",
       "       [150, 155, 305],\n",
       "       [160, 165, 325],\n",
       "       [170, 175, 345],\n",
       "       [180, 185, 365],\n",
       "       [190, 195, 385],\n",
       "       [200, 205, 405],\n",
       "       [210, 215, 425],\n",
       "       [220, 225, 445],\n",
       "       [230, 235, 465],\n",
       "       [240, 245, 485],\n",
       "       [250, 255, 505],\n",
       "       [260, 265, 525],\n",
       "       [270, 275, 545],\n",
       "       [280, 285, 565],\n",
       "       [290, 295, 585]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_seq1 = np.array([x for x in range(0, 300, 10)])\n",
    "in_seq2 = np.array([x for x in range(5, 305, 10)])\n",
    "out_seq = np.array([in_seq1[i] + in_seq2[i] for i in range(len(in_seq1))])\n",
    "\n",
    "\n",
    "# convert to [rows, columns] structure\n",
    "in_seq1 = in_seq1.reshape((len(in_seq1), 1))\n",
    "in_seq2 = in_seq2.reshape((len(in_seq2), 1))\n",
    "out_seq = out_seq.reshape((len(out_seq), 1))\n",
    "\n",
    "# horizontally stack columns\n",
    "dataset = np.hstack((in_seq1, in_seq2, out_seq))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_data = round(len(dataset)*split_ratio)\n",
    "split_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data_shape\n",
      "(21, 3)\n",
      "test_data_shape\n",
      "(9, 3)\n"
     ]
    }
   ],
   "source": [
    "#split data by indexing \n",
    "train_data = dataset[:split_data]\n",
    "test_data = dataset[split_data:]\n",
    "print(\"train_data_shape\")\n",
    "print(train_data.shape)\n",
    "print(\"test_data_shape\")\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "train_data_normalized = scaler.fit_transform(train_data.reshape(-1, 1))\n",
    "test_data_normalized = scaler.fit_transform(test_data.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_data_normalized(21, 3)\n",
      "test_data_normalized(9, 3)\n"
     ]
    }
   ],
   "source": [
    "#transform after scaling\n",
    "train_data_normalized = train_data_normalized.reshape(train_data.shape[0],train_data.shape[1])\n",
    "print(\"test_data_normalized\"+str(train_data_normalized.shape))\n",
    "\n",
    "test_data_normalized = test_data_normalized.reshape(test_data.shape[0],test_data.shape[1])\n",
    "print(\"test_data_normalized\"+str(test_data_normalized.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainX shape:(18, 4, 2) trainY shape:(18,)\n",
      "\n",
      "testX shape:(6, 4, 2) testY shape:(6,)\n"
     ]
    }
   ],
   "source": [
    "trainX ,trainY =  multivariate_univariate_single_step(train_data_normalized,window_size)\n",
    "testX , testY = multivariate_univariate_single_step(test_data_normalized,window_size)\n",
    "print(f\"trainX shape:{trainX.shape} trainY shape:{trainY.shape}\\n\")\n",
    "print(f\"testX shape:{testX.shape} testY shape:{testY.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make training and test sets in torch\n",
    "trainX = torch.from_numpy(trainX).type(torch.Tensor)\n",
    "trainY = torch.from_numpy(trainY).type(torch.Tensor)\n",
    "testX = torch.from_numpy(testX).type(torch.Tensor)\n",
    "testY = torch.from_numpy(testY).type(torch.Tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3D Data Preparation\n",
    "trainX = torch.reshape(trainX,(trainX.shape[0],trainX.shape[1],trainX.shape[2]))\n",
    "trainY = torch.reshape(trainY,(trainY.shape[0],n_step))\n",
    "testX = torch.reshape(testX,(testX.shape[0],trainX.shape[1],trainX.shape[2]))\n",
    "testY = torch.reshape(testY,(testY.shape[0],n_step))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanila LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "\n",
    "        def __init__(self, n_feature, hidden_dim, num_layers, output_dim):\n",
    "            super(LSTM, self).__init__()\n",
    "\n",
    "            self.n_feature = n_feature\n",
    "            # Hidden dimensions\n",
    "            self.hidden_dim = hidden_dim\n",
    "\n",
    "            # Number of hidden layers\n",
    "            self.num_layers = num_layers\n",
    "\n",
    "            # Building your LSTM\n",
    "            # batch_first=True causes input/output tensors to be of shape\n",
    "            # (batch_dim, seq_dim, feature_dim)\n",
    "            self.lstm = nn.LSTM(n_feature, hidden_dim, num_layers, batch_first=True)\n",
    "\n",
    "            # Readout layer\n",
    "            self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "\n",
    "        def forward(self, x):\n",
    "            # Initialize hidden state with zeros\n",
    "            h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "            # Initialize cell state\n",
    "            c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "            # One time step\n",
    "            # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
    "            # If we don't, we'll backprop all the way to the start even after going through another batch\n",
    "            out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "\n",
    "            # Index hidden state of last time step\n",
    "            # out.size() --> 100, 28, 100\n",
    "            # out[:, -1, :] --> 100, 100 --> just want last time step hidden states!\n",
    "            out = self.fc(out[:, -1, :])\n",
    "            # out.size() --> 100, 10\n",
    "            return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BidirectionalLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, n_feature, hidden_dim, num_layers, output_dim):\n",
    "        super(BidirectionalLSTM, self).__init__()\n",
    "\n",
    "        self.n_feature = n_feature\n",
    "        # Hidden dimensions\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Number of hidden layers\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Building your LSTM\n",
    "        # batch_first=True causes input/output tensors to be of shape\n",
    "        # (batch_dim, seq_dim, feature_dim)\n",
    "        self.lstm = nn.LSTM(n_feature, hidden_dim, num_layers, batch_first=True,bidirectional=True)\n",
    "\n",
    "        # Readout layer *2 for bidirectional LSTM\n",
    "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        # Initialize cell state\n",
    "        c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
    "        # If we don't, we'll backprop all the way to the start even after going through another batch\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "\n",
    "        # Index hidden state of last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Arguments for LSTM model\n",
    "hidden_dim = 10\n",
    "number_of_time_series = trainX.shape[2] \n",
    "\n",
    "#1 for vanila LSTM , >1 is mean stacked LSTM\n",
    "num_layers = 1\n",
    "\n",
    "#Vanila , Stacked LSTM\n",
    "# model = LSTM(n_feature=number_of_time_series, hidden_dim=hidden_dim, output_dim=n_step, num_layers=num_layers)\n",
    "\n",
    "#Bidirectional LSTM\n",
    "model = BidirectionalLSTM(n_feature=number_of_time_series, hidden_dim=hidden_dim, output_dim=n_step, num_layers=num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss function \n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "#optimiser\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 MSE:  0.41715627908706665\n",
      "Epoch  1 MSE:  0.3755790591239929\n",
      "Epoch  2 MSE:  0.3406274616718292\n",
      "Epoch  3 MSE:  0.31210410594940186\n",
      "Epoch  4 MSE:  0.2894130349159241\n",
      "Epoch  5 MSE:  0.2716890573501587\n",
      "Epoch  6 MSE:  0.25820520520210266\n",
      "Epoch  7 MSE:  0.24816280603408813\n",
      "Epoch  8 MSE:  0.24015048146247864\n",
      "Epoch  9 MSE:  0.23214925825595856\n",
      "Epoch  10 MSE:  0.22232572734355927\n",
      "Epoch  11 MSE:  0.20983698964118958\n",
      "Epoch  12 MSE:  0.19493837654590607\n",
      "Epoch  13 MSE:  0.17857730388641357\n",
      "Epoch  14 MSE:  0.16195949912071228\n",
      "Epoch  15 MSE:  0.146148219704628\n",
      "Epoch  16 MSE:  0.13169163465499878\n",
      "Epoch  17 MSE:  0.11832424998283386\n",
      "Epoch  18 MSE:  0.10502355545759201\n",
      "Epoch  19 MSE:  0.09063974767923355\n"
     ]
    }
   ],
   "source": [
    "for t in range(num_epochs):\n",
    "    # Initialise hidden state\n",
    "#     Don't do this if you want your LSTM to be stateful\n",
    "#     model.hidden = model.init_hidden()\n",
    "\n",
    "    # Forward pass\n",
    "    y_train_pred = model(trainX)\n",
    "    \n",
    "    #Reshape to perform MSE \n",
    "    y_train_pred=torch.reshape(y_train_pred,(trainY.shape[0],trainY.shape[1]))\n",
    "\n",
    "    loss = loss_fn(y_train_pred, trainY)\n",
    "    print(\"Epoch \", t, \"MSE: \", loss.item())\n",
    "\n",
    "    # Zero out gradient, else they will accumulate between epochs\n",
    "    optimiser.zero_grad()\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Update parameters\n",
    "    optimiser.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "y_test_pred = model(testX)\n",
    "\n",
    "#Reshape to original data\n",
    "y_train_pred = torch.reshape(y_train_pred,(y_train_pred.shape[0],y_train_pred.shape[1]))\n",
    "trainY = torch.reshape(trainY,(trainY.shape[0],trainY.shape[1]))\n",
    "y_test_pred = torch.reshape(y_test_pred,(y_test_pred.shape[0],y_test_pred.shape[1]))\n",
    "testY = torch.reshape(testY,(testY.shape[0],testY.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Invert predictions\n",
    "y_train_pred = scaler.inverse_transform(y_train_pred.detach().numpy())\n",
    "y_train = scaler.inverse_transform(trainY.detach().numpy())\n",
    "y_test_pred = scaler.inverse_transform(y_test_pred.detach().numpy())\n",
    "y_test = scaler.inverse_transform(testY.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y-test\t\t\t\ty-predict\n",
      "[484.99997]\t\t[343.20303]\n",
      "[505.]\t\t[350.80276]\n",
      "[525.]\t\t[358.8228]\n",
      "[545.]\t\t[367.2727]\n",
      "[565.]\t\t[376.1521]\n",
      "[585.]\t\t[385.44693]\n"
     ]
    }
   ],
   "source": [
    "print(\"y-test\\t\\t\\t\\ty-predict\")\n",
    "for i in range(len(y_test_pred)):\n",
    "    print(f\"{y_test[i]}\\t\\t{y_test_pred[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test_shape : (6, 1)\n",
      "y_test_pred_shape : (6, 1)\n"
     ]
    }
   ],
   "source": [
    "print(f\"y_test_shape : {y_test.shape}\")\n",
    "print(f\"y_test_pred_shape : {y_test_pred.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 56.45 RMSE\n",
      "Test Score: 172.52 RMSE\n"
     ]
    }
   ],
   "source": [
    "# calculate root mean squared error\n",
    "trainScore = math.sqrt(mean_squared_error(y_train[:,0], y_train_pred[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(y_test[:,0], y_test_pred[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise for Multivariate (Solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
